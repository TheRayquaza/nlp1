{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87fe3e11-b817-49fd-822b-63cdcd101402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Self, List, Tuple, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import ast\n",
    "from scipy.sparse import hstack\n",
    "import os\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f966e4b-bb85-4aa5-b65c-66f5fe1288d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "additional = {\n",
    "    \"minutes\", \"easiest\", \"ever\", \"aww\", \"i\", \"can\", \"t\", \"believe\", \"it\", \"s\", \"stole\", \"the\", \"idea\", \"from\",\"mirj\", \"andrea\", \" s \", \"andreas\",\n",
    "    \"viestad\", \"andes\", \"andersen\", \"an\", \"ana\", \"amy\", \"2 ww points\", \"on demand\", \"anelia\", \"amazing\",\n",
    "    \"ashley\", \"ashton\", \"amazing\", \"make\", \"house\", \"smell\", \"malcolm\", \"amazingly\", \"killer\", \"perfect\",\n",
    "    \"addictive\", \"leave\", \"u\", \"licking\", \"ur\", \"finger\", \"clean\", \"th\", \"recipe\", \"special\", \"time\", \"favorite\",\n",
    "    \"aunt\", \"jane\", \"soft\", \"and\", \"moist\", \"licking\", \"famous\", \"non fruitcake\", \"true\", \"later\",\n",
    "    \"nonbeliever\", \"believer\", \"comfort\", \"ultimate\", \"lover\", \"love\", \"easy\", \"ugly\", \"cc\", \"uncle\", \"bill\", \"tyler\",\n",
    "    \"unbelievably\", \"unbelievable\", \"healthy\", \"fat\", \"free\", \"un\", \"melt\", \"mouth\", \"ummmmm\", \"umm\", \"ummmy\", \"nummy\", \"ummmm\", \"unattended\",\n",
    "    \"unbaked\", \"ultra\", \"ultimately\", \"yummy\", \"rich\", \"quick\", \"rachael\", \"ray\", \"fail\", \"party\", \"florence\",\n",
    "    \"fast\", \"light\", \"low\", \"carb\", \"snack\", \"wedding\", \"anniversary\", \"anne\", \"marie\", \"annemarie\", \"annette\", \"funicello\", \"syms\",\n",
    "    \"byrn\", \"mike\", \"willan\", \"summer\", \"autumn\", \"winter\", \"spring\", \"burrel\", \"anna\", \"tres\", \"sweet\", \"uber\",\n",
    "    \"homemade\", \"ann\",\"best\",\"j\", \"anite\", \"anitas\", \"anman\", \"angie\", \"angry\", \"simple\", \"difficult\", \"andy\", \"andrew\", \"ancient\", \"still\", \"another\", \"best\", \"go\",\n",
    "    \"grant\", \"grandma\", \"amusement\", \"park\", \"instruction\", \"kitchen\", \"test\", \"ww\", \"almost\", \"empty\", \"dressing\", \"instant\", \"like\", \"le\", \"virtually\",\n",
    "    \"home\", \"made\", \"guilt\", \"guilty\", \"delicious\", \"parfait\", \"forgotten\", \"forget\", \"forevermama\", \"diet\", \"can\", \"real\", \"former\",\n",
    "    \"miss\", \"fabulous\", \"forever\", \"authentic\", \"fortnum\", \"mason\", \"kid\", \"foolproof\", \"football\", \"season\", \"diabetic\",\n",
    "    \"two\", \"small\", \"one\", \"three\", \"four\", \"five\", \"thanksgiving\", \"dream\", \"foothill\", \"paula\", \"deen\", \"food\", \"processor\", \"safari\", \"processor\",\n",
    "    \"traditional\", \"forbidden\", \"flavorful\", \"grandmag\", \"grandmama\", \"grandmaman\", \"grandma\", \"grandmom\", \"lena\", \"alicia\", \"alisa\", \"alice\", \"ali\", \"bit\", \"different\",\n",
    "    \"eat\", \"family\", \"global\", \"gourmet\", \"yam\", \"yam\", \"emotional\", \"balance\", \"tonight\", \"feel\", \"cooking\", \"got\", \"birthday\", \"air\", \"way\", \"mr\", \"never\", \"weep\", \"half\",\n",
    "    \"anything\", \"pour\", \"put\", \"fork\", \"say\", \"stove\", \"top\", \"thought\", \"prize\", \"winning\", \"add\", \"ad\", \"good\", \"better\", \"da\", \"style\", \"even\", \"bran\", \"fake\", \"fire\", \"beautiful\"\n",
    "    \"l\", \"game\", \"day\", \"hate\", \"world\", \"minute\", \"type\", \"starbucks\", \"biggest\", \"dressed\", \"summertime\", \"elmer\", \"johnny\", \"depp\", \"c\", \"p\", \"h\", \"clove\", \"er\", \"star\", \"week\",\n",
    "    \"affair\", \"elegant\", \"student\", \"z\", \"whole\", \"lotta\", \"w\", \"z\", \"b\", \"aaron\", \"craze\", \"a\", \"abc\", \"absolute\", \"absolut\", \"absolutely\", \"perfection\", \"delightful\", \"lazy\", \"morning\",\n",
    "    \"abuelo\", \"abuelito\", \"abuelita\", \"abuela\", \"acadia\", \"accidental\", \"adam\", \"little\", \"interest\", \"addicting\", \"addie\", \"adele\", \"adelaide\", \"adi\", \"adie\", \"adriana\",\n",
    "    \"adult\", \"affordable\", \"alison\", \"holst\", \"purpose\", \"allegheny\", \"allegedly\", \"original\", \"allergic\", \"ex\", \"allergy\", \"allergen\", \"allen\", \"poorman\", \"backyard\",\n",
    "    \"alton\", \"brown\", \"whatever\", \"anthony\", \"anytime\", \"april\", \"fool\", \"ya\", \"fooled\", \"sandra\", \"lee\", \"edna\", \"emma\", \"emy\", \"evy\", \"eva\", 'evelyn', \"fannie\", \"fanny\", \"flo\", \"gladys\", \"helen\", \"grace\", \"ira\", \"irma\",\n",
    "    \"isse\", \"jean\", \"janet\", \"jenny\", \"juju\", \"judy\", \"kathy\", \"kathi\", \"kellie\", \"kelly\", \"laura\", \"lee\", \"kay\", \"kathleen\", \"laura\", \"lee\", \"lesley\", \"lil\", \"linda\", \"liz\", \"lois\", \"louisse\",\n",
    "    \"mag\", 'martguerite', \"margie\", \"marge\", \"maggie\", \"martha\", \"marylin\", \"marion\", \"mary\", \"marthy\", \"melody\", \"michel\", \"meda\", \"millie\", \"muriel\", \"myrna\", \"nelda\", \"nancy\", \"paulie\", \"phillis\", \"rae\", \"rebecca\",\n",
    "    \"rose\", \"sadie\", \"sarah\", \"sara\", \"sue\", \"susan\", \"teresa\", \"theresa\", \"auntie\", \"em\", \"barbara\", \"barb\", \"irene\", \"lolo\", \"lori\", \"lu\", \"maebelle\",\n",
    "    \"aunty\", \"aussie\", \"aurora\", \"austin\", \"l\", \"q\"\n",
    "    \n",
    "    }\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STOPWORDS.update(additional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63308a6f-1c50-4ba9-bf12-bc8363e60b7c",
   "metadata": {},
   "source": [
    "### Baseline Pipeline (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59887066-d1c9-4d37-814d-35cc14d7dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_recipe_names(recipes) -> List:\n",
    "    \"\"\"Basic cleaning and lemmatization of recipe names.\"\"\"\n",
    "    cleaned_recipes = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for recipe in recipes:\n",
    "        recipe = recipe.lower()\n",
    "        recipe = re.sub(r'[^a-z\\s]', '', recipe)\n",
    "        words = recipe.split()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        words = [word for word in words if word not in STOPWORDS]\n",
    "        cleaned_recipes.append(\" \".join(words))\n",
    "    return cleaned_recipes\n",
    "\n",
    "def baseline_pipeline():\n",
    "    data = pd.read_csv(\"../../data/RAW_recipes.csv\")\n",
    "    data.set_index('id', inplace=True)\n",
    "    for col in [\"tags\", \"steps\", \"ingredients\", \"nutrition\"]:\n",
    "        data[col] = data[col].apply(ast.literal_eval)\n",
    "\n",
    "    data.drop(columns=[\"contributor_id\", \"submitted\"], inplace=True, errors=\"ignore\")\n",
    "    data.dropna(subset=[\"name\"], inplace=True)\n",
    "\n",
    "    data['cleaned_name'] = clean_recipe_names(data['name'])\n",
    "    data[\"description\"] = data[\"description\"].apply(lambda x: x.lower() if isinstance(x, str) else \"\")\n",
    "\n",
    "    for col in [\"tags\", \"steps\", \"ingredients\"]:\n",
    "        data[col] = data[col].apply(lambda x: [s.lower() for s in x if isinstance(s, str)] if isinstance(x, list) else [])\n",
    "\n",
    "    data.dropna(subset=[\"tags\", \"steps\", \"ingredients\"], inplace=True, how=\"any\")\n",
    "    data.reset_index(inplace=True)\n",
    "\n",
    "    data['combined_text'] = (\n",
    "        data['cleaned_name'] + ' ' +\n",
    "        data['description'].fillna('') + ' ' +\n",
    "        data['ingredients'].apply(lambda x: \" \".join(x)) + ' ' +\n",
    "        data['steps'].apply(lambda x: \" \".join(x))\n",
    "    )\n",
    "\n",
    "    # TF-IDF vectorizer\n",
    "    print(\"Applying TF-IDF\")\n",
    "    tfidf = TfidfVectorizer(max_features=1000)\n",
    "    X_text = tfidf.fit_transform(data['combined_text'])\n",
    "\n",
    "    # Basic numerical features\n",
    "    X_numeric = data[[\"n_steps\", \"n_ingredients\"]].fillna(0)\n",
    "\n",
    "    X_final = hstack([X_text, X_numeric])\n",
    "\n",
    "    y = data[\"minutes\"]\n",
    "\n",
    "    return X_final, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c972998-71dc-46cb-9e7e-005241af90cd",
   "metadata": {},
   "source": [
    "### Simple Pipeline TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73cf628b-5cc1-431f-9a79-48ae199f6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_recipe_names(recipes) -> List:\n",
    "    \"\"\"Basic cleaning and lemmatization of recipe names.\"\"\"\n",
    "    cleaned_recipes = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for recipe in recipes:\n",
    "        recipe = recipe.lower()\n",
    "        recipe = re.sub(r'[^a-z\\s]', '', recipe)\n",
    "        words = recipe.split()\n",
    "        words = [lemmatizer.lemmatize(word.strip()) for word in words]\n",
    "        words = [word for word in words if word not in STOPWORDS]\n",
    "        cleaned_recipes.append(\" \".join(words))\n",
    "    return cleaned_recipes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def simple_pipeline():\n",
    "    data = pd.read_csv(\"../../data/RAW_recipes.csv\").sample(1000)\n",
    "    data.set_index('id', inplace=True)\n",
    "    for col in [\"tags\", \"steps\", \"ingredients\", \"nutrition\"]:\n",
    "        data[col] = data[col].apply(ast.literal_eval)\n",
    "\n",
    "    data.drop(columns=[\"contributor_id\", \"submitted\"], inplace=True, errors=\"ignore\")\n",
    "    data.dropna(subset=[\"name\"], inplace=True)\n",
    "\n",
    "    data['name'] = clean_recipe_names(data['name'])\n",
    "    for col in [\"tags\", \"steps\", \"ingredients\"]:\n",
    "        data[col] = data[col].apply(lambda x: [s.lower() for s in x if isinstance(s, str)] if isinstance(x, list) else [])\n",
    "\n",
    "    data.dropna(subset=[\"tags\", \"steps\", \"ingredients\"], inplace=True, how=\"any\")\n",
    "    data.reset_index(inplace=True)\n",
    "    \n",
    "    # Join ingredients into one string per recipe\n",
    "    data['ingredients'] = data['ingredients'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    print(\"Applying Bag of Words (CountVectorizer)\")\n",
    "    bow = CountVectorizer(max_features=1000)\n",
    "    X_text = bow.fit_transform(data['ingredients'])\n",
    "\n",
    "    X_numeric = data[[\"n_steps\", \"n_ingredients\"]].fillna(0)\n",
    "\n",
    "    X_final = hstack([X_text, X_numeric])\n",
    "    y = data[\"minutes\"]\n",
    "\n",
    "    return X_final, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1289cc85-7850-4412-9697-acc9e906eef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Bag of Words (CountVectorizer)\n"
     ]
    }
   ],
   "source": [
    "X, y = simple_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e82bbb-9759-4c8f-8137-27a72db0f362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<COOrdinate sparse matrix of dtype 'int64'\n",
       "\twith 17245 stored elements and shape (1000, 1002)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "834ada54-8f01-4588-9eb0-6a9b21b07484",
   "metadata": {},
   "source": [
    "### Analyze on some recipes edge case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba020ee3-6375-4a3f-9345-ebb29c13c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../data/RAW_recipes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d91679d7-5d7b-43df-b63a-c2152552d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92466993-51ea-4c5b-920a-1b6d086c615d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                                  better than sex  strawberries\n",
       "minutes                                                        1460\n",
       "contributor_id                                                41531\n",
       "submitted                                                2002-10-03\n",
       "tags              ['weeknight', 'time-to-make', 'course', 'main-...\n",
       "nutrition             [734.1, 66.0, 199.0, 10.0, 10.0, 117.0, 28.0]\n",
       "n_steps                                                           8\n",
       "steps             ['crush vanilla wafers into fine crumbs and li...\n",
       "description       simple but sexy. this was in my local newspape...\n",
       "ingredients       ['vanilla wafers', 'butter', 'powdered sugar',...\n",
       "n_ingredients                                                     7\n",
       "Name: 42198, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7aa7694-6137-4d3e-8c27-808b0765368f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11        1460\n",
       "12        2970\n",
       "13         525\n",
       "15         500\n",
       "27         495\n",
       "          ... \n",
       "231231     255\n",
       "231354    2900\n",
       "231548     540\n",
       "231572    2895\n",
       "231602     290\n",
       "Name: minutes, Length: 12048, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[(y > 250)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
