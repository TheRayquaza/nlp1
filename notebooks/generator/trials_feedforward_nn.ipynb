{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11732514,"sourceType":"datasetVersion","datasetId":7365204},{"sourceId":382244,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":315649,"modelId":336118},{"sourceId":382956,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":316179,"modelId":336674},{"sourceId":383036,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":316228,"modelId":336734}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0a3f3837-6a57-4ea8-ab60-29bee3e9f231","cell_type":"markdown","source":"# FeedForward Neural Network","metadata":{}},{"id":"2e226fe8-c530-4302-8a6d-4693c15c541e","cell_type":"markdown","source":"## Imports","metadata":{}},{"id":"e5d18cb6-3fd6-4da5-8a46-e6878375e9e2","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport tensorflow as tf\nfrom collections import defaultdict, Counter\nfrom sklearn.model_selection import train_test_split\nfrom typing import List, Tuple, Dict\nimport nltk\nimport os\nimport optuna\nimport sys\nimport pickle\nimport random\nimport ast\nimport math\n\nprint(tf.config.list_physical_devices('GPU'))\n\nsys.path.append(os.path.abspath(\"/kaggle/input/recipebatchfeedforwardgenerator/tensorflow2/default/1\"))\nsys.path.append(os.path.abspath(\"/kaggle/input/optimizedffnnwithcustomloss/keras/default/1/optimized_feedforward_nn.py\"))\n\n#from feedforward_nn import BatchFeedForwardNN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:48:34.441629Z","iopub.execute_input":"2025-05-09T14:48:34.441895Z","iopub.status.idle":"2025-05-09T14:48:34.447719Z","shell.execute_reply.started":"2025-05-09T14:48:34.441875Z","shell.execute_reply":"2025-05-09T14:48:34.446849Z"}},"outputs":[{"name":"stdout","text":"[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n","output_type":"stream"}],"execution_count":47},{"id":"0f17df01-e80f-4048-8bd7-95d476f75dd7","cell_type":"markdown","source":"## ⭐ Base Preprocessing","metadata":{}},{"id":"8a26f553-53f9-4f5e-8dbb-6cdaa3bfaca5","cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/preprocessed-recipe/preprocessed_recipe.csv\")\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:48:34.951427Z","iopub.execute_input":"2025-05-09T14:48:34.952046Z","iopub.status.idle":"2025-05-09T14:48:36.428772Z","shell.execute_reply.started":"2025-05-09T14:48:34.952026Z","shell.execute_reply":"2025-05-09T14:48:36.427965Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0      id                          name  minutes  n_steps  \\\n0           0  137739   arriba baked squash mexican       55       11   \n1           1   31490               breakfast pizza       30        9   \n2           4   44061  amish tomato ketchup canning      190        5   \n3           5   25274               marinated olive       15        4   \n4           6   67888                 barbecued rib      120       10   \n\n                                         description  n_ingredients  \\\n0  autumn is my favorite time of year to cook! th...              7   \n1  this recipe calls for the crust to be prebaked...              6   \n2  my dh's amish mother raised him on this recipe...              8   \n3  my italian mil was thoroughly impressed by my ...              9   \n4  this recipe is posted by request and was origi...             22   \n\n                            steps_string_standardize  \\\n0  make a choic and proceed with recip depend on ...   \n1  preheat oven to 103.33 celsius °c press dough ...   \n2  mix all ingredients& boil for 2 30.0 minute , ...   \n3  toast the fennel seed and lightli crush them p...   \n4  in a medium saucepan combin all the ingredi fo...   \n\n                                    ingredients_text  \\\n0  ['winter squash', 'mexican seasoning', 'mixed ...   \n1  ['prepared pizza crust', 'sausage patty', 'egg...   \n2  ['tomato juice', 'apple cider vinegar', 'sugar...   \n3  ['fennel seeds', 'green olives', 'ripe olives'...   \n4  ['pork spareribs', 'soy sauce', 'fresh garlic'...   \n\n                                           tags_text  \\\n0  ['60-minutes-or-less', 'time-to-make', 'course...   \n1  ['30-minutes-or-less', 'time-to-make', 'course...   \n2  ['weeknight', 'time-to-make', 'course', 'main-...   \n3  ['15-minutes-or-less', 'time-to-make', 'course...   \n4  ['weeknight', 'time-to-make', 'course', 'main-...   \n\n                         cuisine  calories  total_fat  sugar  sodium  protein  \\\n0  North America – United States      51.5        0.0   13.0     0.0      2.0   \n1  North America – United States     173.4       18.0    0.0    17.0     22.0   \n2  North America – United States     352.9        1.0  337.0    23.0      3.0   \n3  North America – United States     380.7       53.0    7.0    24.0      6.0   \n4  North America – United States    1109.5       83.0  378.0   275.0     96.0   \n\n   saturated_fat  carbohydrates  \n0            0.0            4.0  \n1           35.0            1.0  \n2            0.0           28.0  \n3           24.0            6.0  \n4           86.0           36.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>name</th>\n      <th>minutes</th>\n      <th>n_steps</th>\n      <th>description</th>\n      <th>n_ingredients</th>\n      <th>steps_string_standardize</th>\n      <th>ingredients_text</th>\n      <th>tags_text</th>\n      <th>cuisine</th>\n      <th>calories</th>\n      <th>total_fat</th>\n      <th>sugar</th>\n      <th>sodium</th>\n      <th>protein</th>\n      <th>saturated_fat</th>\n      <th>carbohydrates</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>137739</td>\n      <td>arriba baked squash mexican</td>\n      <td>55</td>\n      <td>11</td>\n      <td>autumn is my favorite time of year to cook! th...</td>\n      <td>7</td>\n      <td>make a choic and proceed with recip depend on ...</td>\n      <td>['winter squash', 'mexican seasoning', 'mixed ...</td>\n      <td>['60-minutes-or-less', 'time-to-make', 'course...</td>\n      <td>North America – United States</td>\n      <td>51.5</td>\n      <td>0.0</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>31490</td>\n      <td>breakfast pizza</td>\n      <td>30</td>\n      <td>9</td>\n      <td>this recipe calls for the crust to be prebaked...</td>\n      <td>6</td>\n      <td>preheat oven to 103.33 celsius °c press dough ...</td>\n      <td>['prepared pizza crust', 'sausage patty', 'egg...</td>\n      <td>['30-minutes-or-less', 'time-to-make', 'course...</td>\n      <td>North America – United States</td>\n      <td>173.4</td>\n      <td>18.0</td>\n      <td>0.0</td>\n      <td>17.0</td>\n      <td>22.0</td>\n      <td>35.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>44061</td>\n      <td>amish tomato ketchup canning</td>\n      <td>190</td>\n      <td>5</td>\n      <td>my dh's amish mother raised him on this recipe...</td>\n      <td>8</td>\n      <td>mix all ingredients&amp; boil for 2 30.0 minute , ...</td>\n      <td>['tomato juice', 'apple cider vinegar', 'sugar...</td>\n      <td>['weeknight', 'time-to-make', 'course', 'main-...</td>\n      <td>North America – United States</td>\n      <td>352.9</td>\n      <td>1.0</td>\n      <td>337.0</td>\n      <td>23.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>25274</td>\n      <td>marinated olive</td>\n      <td>15</td>\n      <td>4</td>\n      <td>my italian mil was thoroughly impressed by my ...</td>\n      <td>9</td>\n      <td>toast the fennel seed and lightli crush them p...</td>\n      <td>['fennel seeds', 'green olives', 'ripe olives'...</td>\n      <td>['15-minutes-or-less', 'time-to-make', 'course...</td>\n      <td>North America – United States</td>\n      <td>380.7</td>\n      <td>53.0</td>\n      <td>7.0</td>\n      <td>24.0</td>\n      <td>6.0</td>\n      <td>24.0</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>67888</td>\n      <td>barbecued rib</td>\n      <td>120</td>\n      <td>10</td>\n      <td>this recipe is posted by request and was origi...</td>\n      <td>22</td>\n      <td>in a medium saucepan combin all the ingredi fo...</td>\n      <td>['pork spareribs', 'soy sauce', 'fresh garlic'...</td>\n      <td>['weeknight', 'time-to-make', 'course', 'main-...</td>\n      <td>North America – United States</td>\n      <td>1109.5</td>\n      <td>83.0</td>\n      <td>378.0</td>\n      <td>275.0</td>\n      <td>96.0</td>\n      <td>86.0</td>\n      <td>36.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":48},{"id":"9bb82bcd-7848-4541-bc91-1f6877319703","cell_type":"markdown","source":"## Simple FFNN\n\nThis is a simple feed forward neural network that learns to predict the next token using a fixed context\n\n#### Training Process (context of 3)\n\nInput: `<s> ing1 ing2 <STEPS> w1 w2 </s>`\n\n| Sample | Input (`x`)                 | Target token (`y`) |\n|------|-----------------------------|--------------|\n| x1   | `<s> ing1 ing2`             | `<STEPS>`    |\n| x2   | `ing1 ing2 <STEPS>`         | `w1`         |\n| x3   | `ing2 <STEPS> w1`           | `w2`         |\n| x4   | `<STEPS> w1 w2`             | `</s>`       |\n\nUnknown words are encoded as `<UNKNOWN>`\n\n#### Model Architecture\n\n- One Hot encoding (`vocab_size`)\n- Embdedding layer (`vocab_size`, `embedding_dim`)\n- Dense layer (`embedding_dim`, `hidden_dim`)\n- Dense layer (`hidden_dim`, `vocab_size`)\n- Softmax layer (`vocab_size`)","metadata":{}},{"id":"a6643fed-3110-4185-853d-0e29238a2d8b","cell_type":"markdown","source":"#### Preprocessing","metadata":{}},{"id":"e987f0bd-f5e6-43a0-8e59-de0a66790f3d","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom typing import List, Dict, Set, Tuple\nimport string\nimport pickle\nimport warnings\nimport ast\nfrom collections import Counter, defaultdict\nfrom tensorflow.keras.preprocessing import text\n\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\n\nclass KerasTokenizerGeneratorPreprocessing:\n    def __init__(self,\n                 ingredients_col='ingredients_text',\n                 steps_col='steps_string_standardize',\n                 drop_uncommon=None,\n                 max_num_words=10000):\n        self.ingredients_col = ingredients_col\n        self.steps_col = steps_col\n        self.stemmer = LancasterStemmer()\n        self.drop_uncommon = drop_uncommon\n        self.max_num_words = max_num_words\n        self.tokenizer = text.Tokenizer(num_words=self.max_num_words)\n\n    def _extract_ingredients(self, row) -> List[str]:\n        if pd.isna(row[self.ingredients_col]):\n            return []\n\n        ingredients = ast.literal_eval(row[self.ingredients_col])\n        return [self.stemmer.stem(ing) for ing in ingredients if ing]\n\n    def _extract_steps(self, row) -> List[str]:\n        if pd.isna(row[self.steps_col]):\n            return []\n    \n        steps = row[self.steps_col].split(',')\n        steps = [step.strip().lower() for step in steps]\n    \n        cleaned_stems = []\n        for step in steps:\n            cleaned = re.sub(r'[^a-zA-Z0-9 .]', '', step)\n    \n            if not cleaned or cleaned.isdigit():\n                continue\n    \n            stemmed = self.stemmer.stem(cleaned)\n            cleaned_stems.append(stemmed)\n    \n        return cleaned_stems\n    \n    def _collect_corpus(self, X: pd.DataFrame) -> List[str]:\n        corpus = []\n        for _, row in X.iterrows():\n            ingredients = self._extract_ingredients(row)\n            steps = self._extract_steps(row)\n            \n            if ingredients:\n                corpus.extend(ingredients)\n            if steps:\n                corpus.extend(steps)\n        return corpus\n\n    def tokenize(self, text: str) -> List[str]:\n        return text.split()\n    \n    def tokenize_list(self, text_list: List[str]) -> List[List[str]]:\n        flattened = []\n        for text in text_list:\n            flattened.extend(self.tokenize(text))\n        return flattened\n\n    def fit(self, X: pd.DataFrame, y=None):\n        missing_cols = [col for col in [self.ingredients_col, self.steps_col] if col not in X.columns]\n        if missing_cols:\n            raise ValueError(f\"Columns {missing_cols} not found in the DataFrame\")\n        \n        # Collect all text to fit the tokenizer\n        all_steps = []\n        for _, row in X.iterrows():\n            steps = self._extract_steps(row)\n            all_steps.extend(steps)\n        \n        # Fit the tokenizer on the corpus\n        self.tokenizer.fit_on_texts(all_steps)\n        return self\n\n    def transform(self, X: pd.DataFrame) -> Tuple[List[List[str]], List[List[str]]]:\n        X = X.dropna(subset=[self.ingredients_col, self.steps_col])\n        ingredients_lists = []\n        steps_lists = []\n        ingredients_counter = defaultdict(int)\n\n        for _, row in X.iterrows():\n            ingredients = self._extract_ingredients(row)\n            steps = self._extract_steps(row)\n            \n            for ing in ingredients:\n                ingredients_counter[ing] += 1\n                \n            ingredients_lists.append(ingredients)\n            steps_lists.append(steps)\n\n        if self.drop_uncommon is not None:\n            uncommon_ingredients = {\n                ing for ing, count in ingredients_counter.items()\n                if count < self.drop_uncommon\n            }\n    \n            filtered_ingredients_lists = []\n            filtered_steps_lists = []\n            for ingredients, steps in zip(ingredients_lists, steps_lists):\n                if not any(ing in uncommon_ingredients for ing in ingredients):\n                    filtered_ingredients_lists.append(ingredients)\n                    filtered_steps_lists.append(steps)\n    \n            ingredients_lists = filtered_ingredients_lists\n            steps_lists = filtered_steps_lists\n\n        tokenized_steps_lists = []\n        for steps in steps_lists:\n            all_words = []\n            for step in steps:\n                all_words.extend(step.split())\n            tokenized_steps_lists.append(all_words)\n\n        return ingredients_lists, tokenized_steps_lists\n\n    def fit_transform(self, X: pd.DataFrame, y=None) -> Tuple[List[List[str]], List[List[str]]]:\n        return self.fit(X).transform(X)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:48:36.561220Z","iopub.execute_input":"2025-05-09T14:48:36.561490Z","iopub.status.idle":"2025-05-09T14:48:36.578149Z","shell.execute_reply.started":"2025-05-09T14:48:36.561469Z","shell.execute_reply":"2025-05-09T14:48:36.577336Z"}},"outputs":[],"execution_count":49},{"id":"3964f9f7-140b-491d-a5f3-08d35e0425d5","cell_type":"code","source":"keras_generator_preprocessing = KerasTokenizerGeneratorPreprocessing()\ningredients, steps = keras_generator_preprocessing.fit_transform(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:48:38.601108Z","iopub.execute_input":"2025-05-09T14:48:38.601387Z","iopub.status.idle":"2025-05-09T14:49:16.014528Z","shell.execute_reply.started":"2025-05-09T14:48:38.601368Z","shell.execute_reply":"2025-05-09T14:49:16.013961Z"}},"outputs":[],"execution_count":50},{"id":"cb41b266-9470-4429-8a16-14d25ebed8e0","cell_type":"code","source":"ingredients[0], steps[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T10:47:16.648744Z","iopub.execute_input":"2025-05-09T10:47:16.649346Z","iopub.status.idle":"2025-05-09T10:47:16.654732Z","shell.execute_reply.started":"2025-05-09T10:47:16.649323Z","shell.execute_reply":"2025-05-09T10:47:16.654119Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(['winter squash',\n  'mexican seasoning',\n  'mixed spice',\n  'honey',\n  'but',\n  'olive oil',\n  'salt'],\n ['make',\n  'a',\n  'choic',\n  'and',\n  'proceed',\n  'with',\n  'recip',\n  'depend',\n  'on',\n  'size',\n  'of',\n  'squash',\n  'cut',\n  'into',\n  'half',\n  'or',\n  'fourth',\n  'remov',\n  'seed',\n  'for',\n  'spici',\n  'squash',\n  'drizzl',\n  'oliv',\n  'oil',\n  'or',\n  'melt',\n  'butter',\n  'over',\n  'each',\n  'cut',\n  'squash',\n  'piec',\n  'season',\n  'with',\n  'mexican',\n  'season',\n  'mix',\n  'ii',\n  'for',\n  'sweet',\n  'squash',\n  'drizzl',\n  'melt',\n  'honey',\n  'but',\n  'grate',\n  'piloncillo',\n  'over',\n  'each',\n  'cut',\n  'squash',\n  'piec',\n  'season',\n  'with',\n  'sweet',\n  'mexican',\n  'spice',\n  'mix',\n  'bake',\n  'at',\n  '176.67',\n  'celsius',\n  'again',\n  'depend',\n  'on',\n  'size',\n  'for',\n  '40.0',\n  'minute',\n  'up',\n  'to',\n  'an',\n  'ho',\n  'until',\n  'a',\n  'fork',\n  'can',\n  'easili',\n  'pierc',\n  'the',\n  'skin',\n  'be',\n  'care',\n  'not',\n  'to',\n  'burn',\n  'the',\n  'squash',\n  'especi',\n  'if',\n  'you',\n  'opt',\n  'to',\n  'use',\n  'sugar',\n  'or',\n  'butter',\n  'if',\n  'you',\n  'feel',\n  'more',\n  'comfort',\n  'cover',\n  'the',\n  'squash',\n  'with',\n  'aluminum',\n  'foil',\n  'the',\n  'first',\n  'half',\n  'ho',\n  'give',\n  'or',\n  'tak',\n  'of',\n  'bake',\n  'if',\n  'desir',\n  'season',\n  'with',\n  'salt'])"},"metadata":{}}],"execution_count":7},{"id":"7a18416a-c8c7-4164-bc67-dea8afaa4107","cell_type":"markdown","source":"## Optimized FNN","metadata":{}},{"id":"eb6cf2ea-b5eb-4a54-a876-dd9152503444","cell_type":"markdown","source":"#### Training","metadata":{}},{"id":"814b3859-d784-4a6a-a56d-d52446b98b85","cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nfrom typing import List, Tuple, Dict\nfrom collections import Counter, defaultdict\nimport time\n\nclass OptimizedBatchFeedForwardNN:\n    def __init__(self):\n        self.model = None\n        self.vocab = None\n        self.token_to_id = None\n        self.id_to_token = None\n        self.context = None\n        self.batch_size = 32\n\n    def _build_vocabulary(self, ingredients: List[List[str]], steps: List[List[str]]) -> None:\n        print(\"Building vocabulary...\")\n        all_tokens = [token for lst in ingredients + steps for token in lst] + ['<UNKNOWN>', '<s>', '</s>', \"<STEPS>\"]\n        counter = Counter(all_tokens)\n        self.vocab = [token for token, _ in sorted(counter.items(), key=lambda x: (-x[1], x[0]))]\n        self.token_to_id = {token: idx for idx, token in enumerate(self.vocab)}\n        self.id_to_token = {idx: token for idx, token in enumerate(self.vocab)}\n\n    def _prepare_train(self, ingredients: List[List[str]], steps: List[List[str]]) -> Tuple[np.ndarray, np.ndarray]:        \n        if self.vocab is None:\n            self._build_vocabulary(ingredients, steps)\n\n        context_targets = defaultdict(Counter)\n        \n        for i in range(len(ingredients)):\n            ingredient_ids = [self.token_to_id[\"<s>\"]] + \\\n                             [self.token_to_id.get(token, self.token_to_id['<UNKNOWN>']) for token in ingredients[i]] + \\\n                             [self.token_to_id['<STEPS>']]\n            step_ids = [self.token_to_id.get(step_token, self.token_to_id['<UNKNOWN>']) for step_token in steps[i]] + \\\n                       [self.token_to_id[\"</s>\"]]\n\n            tokens_ids = ingredient_ids + step_ids\n\n            for k in range(len(tokens_ids) - self.context):\n                context_window = tuple(tokens_ids[k:k+self.context])\n                context_targets[context_window][tokens_ids[k+self.context]] += 1\n\n        print(f\"Found {len(context_targets)} unique context-grams\")\n\n        X_data = []\n        y_data = []\n        sample_weights = []\n\n        print(\"Building sample weights using context gram...\")\n        for context_gram, target_counts in context_targets.items():\n            total_count = sum(target_counts.values())\n            for target_id, count in target_counts.items():\n                X_data.append(list(context_gram))\n                y_data.append(target_id)\n                sample_weights.append(count / total_count)\n\n        return np.array(X_data, dtype=np.int32), np.array(y_data, dtype=np.int32), np.array(sample_weights, dtype=np.float32)\n        \n    def fit(self, ingredients: List[List[str]], steps: List[List[str]],\n            embedding_dim=256, hidden_dim=512, context=3,\n            epochs=10, batch_size=32, validation_split=0.1, dropout=None,\n            learning_rate=1e-2, custom_loss=None\n    ):\n        if len(ingredients) != len(steps):\n            raise ValueError(f\"dimension mismatch {len(ingredients)} vs {len(steps)}\")\n\n        start_time = time.time()\n        self.batch_size = batch_size\n        self.context = context\n\n        print(\"Preparing unique context-grams for training...\")\n        X_train, y_train, sample_weights = self._prepare_train(ingredients, steps)\n\n        vocab_size = len(self.vocab)\n        print(f\"Vocab size: {vocab_size}\")\n        print(f\"Unique training samples: {len(X_train)}\")\n        print(f\"Data preparation took {time.time() - start_time:.2f} seconds\")\n        for i in range(10):\n            print(f\"Sample {i}: {X_train[i]} - {y_train[i]}\")\n\n        print(\"Building model...\")\n        self.model = keras.Sequential()\n        self.model.add(keras.layers.Input(shape=(context,), dtype=tf.int32))\n        self.model.add(keras.layers.Embedding(vocab_size * context, embedding_dim, name=\"embedding\"))\n        self.model.add(keras.layers.Flatten())\n        self.model.add(keras.layers.BatchNormalization())\n        self.model.add(keras.layers.Dense(hidden_dim, activation='relu', name=\"hidden\"))\n\n        if dropout is not None:\n            self.model.add(keras.layers.Dropout(dropout))\n        self.model.add(keras.layers.BatchNormalization())\n        self.model.add(keras.layers.Dense(vocab_size, activation='softmax', name=\"output\"))\n\n        self.model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n            loss=(custom_loss if custom_loss != None else tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)),\n            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n        )\n\n        self.model.build(input_shape=(None, context))\n\n        print(self.model.summary())\n        \n        train_dataset = tf.data.Dataset.from_tensor_slices(\n            (X_train, y_train, sample_weights)\n        ).shuffle(buffer_size=10000)\n        \n        val_size = int(len(X_train) * validation_split)\n        train_size = len(X_train) - val_size\n        \n        train_ds = train_dataset.take(train_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n        val_ds = train_dataset.skip(train_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n        start_train_time = time.time()\n        history = self.model.fit(\n            train_ds,\n            epochs=epochs,\n            validation_data=val_ds,\n            callbacks= [\n                tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n                tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=2)\n            ]\n        )\n\n        print(f\"Training took {time.time() - start_train_time:.2f} seconds\")\n        print(f\"Total process took {time.time() - start_time:.2f} seconds\")\n\n        return self\n\n    def _prepare_test(self, ingredients: List[List[str]]):\n        if self.vocab is None:\n            raise ValueError(\"Vocabulary not built. Fit the model first!\")\n        \n        all_contexts = []\n        for ingredient_list in ingredients:\n            ingredient_ids = [self.token_to_id.get(token, self.token_to_id['<UNKNOWN>']) \n                              for token in ingredient_list] + [self.token_to_id['<STEPS>']]\n            \n            if len(ingredient_ids) >= self.context - 1:\n                context_tokens = ingredient_ids[-(self.context - 1):]\n            else:\n                padding = [self.token_to_id['<UNKNOWN>']] * (self.context - 1 - len(ingredient_ids))\n                context_tokens = padding + ingredient_ids\n            \n            all_contexts.append(context_tokens)\n        \n        return np.array(all_contexts)\n\n    def predict(self, ingredients: List[List[str]], max_steps=20) -> List[List[str]]:\n        if self.model is None:\n            raise ValueError(\"Model not fitted!\")\n    \n        all_contexts = self._prepare_test(ingredients)\n    \n        num_ingredients = len(ingredients)\n        current_tokens = [['<s>'] for _ in range(num_ingredients)]\n        completed = [False] * num_ingredients\n        \n        for step in range(max_steps):\n            if all(completed):\n                break\n            \n            active_indices = [i for i, is_complete in enumerate(completed) if not is_complete]\n            \n            # Process in batches of self.batch_size\n            for batch_start in range(0, len(active_indices), self.batch_size):\n                batch_indices = active_indices[batch_start:batch_start + self.batch_size]\n                \n                batch_contexts = []\n                for idx in batch_indices:\n                    if len(current_tokens[idx]) == 1:\n                        context = all_contexts[idx]\n                    else:\n                        recent_tokens = [self.token_to_id.get(token, self.token_to_id['<UNKNOWN>']) \n                                         for token in current_tokens[idx][1:]]\n                        context_size = self.context - 1\n                        \n                        if len(recent_tokens) >= context_size:\n                            context = recent_tokens[-context_size:]\n                        else:\n                            orig_context_needed = context_size - len(recent_tokens)\n                            context = list(all_contexts[idx][-orig_context_needed:]) + recent_tokens\n                    \n                    batch_contexts.append(context)\n                \n                batch_input = np.array(batch_contexts)\n                \n                predictions = self.model.predict_on_batch(batch_input)\n                \n                for i, pred_idx in enumerate(batch_indices):\n                    next_token_id = np.argmax(predictions[i])\n                    next_token = self.id_to_token[next_token_id]\n                    \n                    current_tokens[pred_idx].append(next_token)\n                    \n                    if next_token == '</s>':\n                        completed[pred_idx] = True\n        \n        all_predictions = []\n        for tokens in current_tokens:\n            if tokens[0] == '<s>':\n                tokens = tokens[1:]\n            \n            if tokens and tokens[-1] == '</s>':\n                tokens = tokens[:-1]\n                \n            all_predictions.append(tokens)\n        \n        return all_predictions\n\n    def save(self, filepath: str):\n        if self.model is None:\n            raise ValueError(\"No model to save!\")\n\n        self.model.save(filepath)\n\n        np.savez(f\"{filepath}_vocab.npz\", \n                 vocab=np.array(self.vocab, dtype=object),\n                 context=np.array([self.context]),\n                 batch_size=np.array([self.batch_size]),\n        )\n\n    def load(self, filepath: str):\n        self.model = keras.models.load_model(filepath)\n\n        data = np.load(f\"{filepath}_vocab.npz\", allow_pickle=True)\n\n        self.vocab = data['vocab'].tolist()\n        self.batch_size = int(data['batch_size'][0])\n        self.context = int(data['context'][0])\n        self.token_to_id = {token: idx for idx, token in enumerate(self.vocab)}\n        self.id_to_token = {idx: token for idx, token in enumerate(self.vocab)}\n        return self","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:49:16.045195Z","iopub.execute_input":"2025-05-09T14:49:16.045389Z","iopub.status.idle":"2025-05-09T14:49:16.081491Z","shell.execute_reply.started":"2025-05-09T14:49:16.045375Z","shell.execute_reply":"2025-05-09T14:49:16.080837Z"},"scrolled":true},"outputs":[],"execution_count":51},{"id":"340c8b55-8b5f-4bd7-b441-5dc201102750","cell_type":"code","source":"model = OptimizedBatchFeedForwardNN()\nmodel.fit(ingredients, steps, \n          embedding_dim=512, hidden_dim=256, \n          context=8, epochs=5, dropout=0.2, \n          batch_size=2048, learning_rate=1e-1,\n)\nmodel.save(\"/kaggle/working/kerastok_optimized_ffnn_512_embedding_256_hidden_8_context_1e-1_lr.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T15:14:40.824612Z","iopub.execute_input":"2025-05-09T15:14:40.825209Z","iopub.status.idle":"2025-05-09T15:46:36.965112Z","shell.execute_reply.started":"2025-05-09T15:14:40.825184Z","shell.execute_reply":"2025-05-09T15:46:36.964523Z"}},"outputs":[{"name":"stdout","text":"Preparing unique context-grams for training...\nBuilding vocabulary...\nFound 8419399 unique context-grams\nBuilding sample weights using context gram...\nVocab size: 39734\nUnique training samples: 8570864\nData preparation took 45.26 seconds\nSample 0: [22750  6184  8942  3087   234    51   119    15] - 22748\nSample 1: [ 6184  8942  3087   234    51   119    15 22748] - 91\nSample 2: [ 8942  3087   234    51   119    15 22748    91] - 2\nSample 3: [ 3087   234    51   119    15 22748    91     2] - 914\nSample 4: [  234    51   119    15 22748    91     2   914] - 1\nSample 5: [   51   119    15 22748    91     2   914     1] - 1970\nSample 6: [  119    15 22748    91     2   914     1  1970] - 5\nSample 7: [   15 22748    91     2   914     1  1970     5] - 550\nSample 8: [22748    91     2   914     1  1970     5   550] - 696\nSample 9: [  91    2  914    1 1970    5  550  696] - 11\nBuilding model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_13\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)              │     \u001b[38;5;34m162,750,464\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_13 (\u001b[38;5;33mFlatten\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_26               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │          \u001b[38;5;34m16,384\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ hidden (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │       \u001b[38;5;34m1,048,832\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_27               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │           \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ output (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m39734\u001b[0m)               │      \u001b[38;5;34m10,211,638\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │     <span style=\"color: #00af00; text-decoration-color: #00af00\">162,750,464</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_26               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,832</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_27               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39734</span>)               │      <span style=\"color: #00af00; text-decoration-color: #00af00\">10,211,638</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m174,028,342\u001b[0m (663.87 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">174,028,342</span> (663.87 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m174,019,638\u001b[0m (663.83 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">174,019,638</span> (663.83 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8,704\u001b[0m (34.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,704</span> (34.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 162750464 elements. This may consume a large amount of memory.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3767/3767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 98ms/step - loss: 4.5940 - sparse_categorical_accuracy: 0.2416 - val_loss: 5.0003 - val_sparse_categorical_accuracy: 0.2230 - learning_rate: 0.1000\nEpoch 2/5\n\u001b[1m3767/3767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 99ms/step - loss: 4.5697 - sparse_categorical_accuracy: 0.2203 - val_loss: 4.2193 - val_sparse_categorical_accuracy: 0.2669 - learning_rate: 0.1000\nEpoch 3/5\n\u001b[1m3767/3767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 98ms/step - loss: 6.1209 - sparse_categorical_accuracy: 0.2319 - val_loss: 6.4841 - val_sparse_categorical_accuracy: 0.1543 - learning_rate: 0.1000\nEpoch 4/5\n\u001b[1m3767/3767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 99ms/step - loss: 5.0623 - sparse_categorical_accuracy: 0.2402 - val_loss: 6.5508 - val_sparse_categorical_accuracy: 0.1571 - learning_rate: 0.1000\nEpoch 5/5\n\u001b[1m3767/3767\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 98ms/step - loss: 4.0447 - sparse_categorical_accuracy: 0.2619 - val_loss: 4.0902 - val_sparse_categorical_accuracy: 0.2840 - learning_rate: 0.0200\nTraining took 1861.53 seconds\nTotal process took 1907.35 seconds\n","output_type":"stream"}],"execution_count":54},{"id":"abe88cd4-926d-433f-b019-544a3ff68cdd","cell_type":"markdown","source":"## BPE FFNN","metadata":{}},{"id":"62773ce7-8117-461f-8186-7c4c688ddaab","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom typing import List, Dict, Set, Tuple\nimport string\nimport pickle\nimport warnings\nimport ast\nfrom collections import Counter, defaultdict\nfrom transformers import AutoTokenizer\n\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\n\nclass BPEGeneratorPreprocessing:\n    def __init__(self,\n                 ingredients_col='ingredients_text',\n                 steps_col='steps_string_standardize',\n                 drop_uncommon=None,\n                 vocab_size=256):\n        self.ingredients_col = ingredients_col\n        self.steps_col = steps_col\n        self.stemmer = LancasterStemmer()\n        self.drop_uncommon = drop_uncommon\n        self.vocab_size = vocab_size\n        \n        self.vocab = []\n        self.merges = {}\n        self.word_freqs = defaultdict(int)\n        self.splits = {}\n\n    def get_savepath(self, prefix: str):\n        return f\"{prefix}bpe_preprocessing_{self.vocab_size}_vocab_size\";\n\n    def _extract_ingredients(self, row) -> List[str]:\n        if pd.isna(row[self.ingredients_col]):\n            print(\"  - No ingredients found (NaN value)\")\n            return []\n\n        ingredients = ast.literal_eval(row[self.ingredients_col])\n        stemmed = [self.stemmer.stem(ing) for ing in ingredients if ing]\n        return stemmed\n\n    def _extract_steps(self, row) -> List[str]:\n        if pd.isna(row[self.steps_col]):\n            print(\"  - No steps found (NaN value)\")\n            return []\n    \n        steps = row[self.steps_col].split(',')\n        steps = [step.strip().lower() for step in steps]\n    \n        cleaned_stems = []\n        for step in steps:\n            cleaned = re.sub(r'[^a-zA-Z0-9 .]', '', step)\n    \n            if not cleaned or cleaned.isdigit():\n                continue\n    \n            stemmed = self.stemmer.stem(cleaned)\n            cleaned_stems.append(stemmed)\n    \n        return cleaned_stems\n    \n    def _collect_corpus(self, X: pd.DataFrame) -> List[str]:\n        print(f\"\\nCollecting corpus from {len(X)} rows...\")\n        corpus = []\n        for idx, row in X.iterrows():\n            if idx % 100 == 0 and idx > 0:\n                print(f\"  - Processed {idx} rows, corpus size: {len(corpus)}\")\n            \n            ingredients = self._extract_ingredients(row)\n            steps = self._extract_steps(row)\n            \n            if ingredients:\n                corpus.extend(ingredients)\n            if steps:\n                corpus.extend(steps)\n                \n        print(f\"Final corpus size: {len(corpus)} tokens\")\n        print(f\"Sample corpus entries: {corpus[:5]}\")\n        return corpus\n\n    def _train_bpe(self, corpus: List[str]):\n        print(\"\\nTraining BPE tokenizer...\")\n        print(\"Building initial word frequencies...\")\n        for text in corpus:\n            self.word_freqs[text] += 1\n        \n        print(f\"Unique words in corpus: {len(self.word_freqs)}\")\n        print(f\"Top 5 most common words: {Counter(self.word_freqs).most_common(5)}\")\n        \n        print(\"\\nBuilding initial alphabet...\")\n        alphabet = set()\n        for word in self.word_freqs.keys():\n            for letter in word:\n                alphabet.add(letter)\n        \n        print(f\"Alphabet size: {len(alphabet)}\")\n        \n        self.vocab = [\"<s>\", \"</s>\", \"<RECIPE>\"] + sorted(list(alphabet))\n        print(f\"Initial vocabulary size: {len(self.vocab)}\")\n        \n        print(\"\\nInitializing splits (words into characters)...\")\n        self.splits = {word: [c for c in word] for word in self.word_freqs.keys()}\n        \n        # Apply BPE algorithm\n        print(\"\\nStarting BPE merge operations...\")\n        iteration = 0\n        while len(self.vocab) < self.vocab_size:\n            iteration += 1\n            print(f\"\\nIteration {iteration}: Current vocab size = {len(self.vocab)}\")\n            \n            print(\"  Computing pair frequencies...\")\n            pair_freqs = self._compute_pair_freqs()\n            if not pair_freqs:\n                print(\"  No more pairs to merge, stopping early\")\n                break\n                \n            best_pair = max(pair_freqs.items(), key=lambda x: x[1])[0]\n            best_pair_freq = pair_freqs[best_pair]\n            print(f\"  Best pair: ('{best_pair[0]}', '{best_pair[1]}') with frequency {best_pair_freq}\")\n            \n            print(f\"  Merging pair '{best_pair[0]}' + '{best_pair[1]}'...\")\n            self.splits = self._merge_pair(best_pair[0], best_pair[1])\n            \n            self.merges[best_pair] = best_pair[0] + best_pair[1]\n            self.vocab.append(best_pair[0] + best_pair[1])\n            \n            if iteration % 100 == 0:\n                print(f\"  Current vocab size: {len(self.vocab)}\")\n                print(f\"  Latest 5 tokens: {self.vocab[-5:]}\")\n        \n        print(f\"\\nBPE training complete. Final vocabulary size: {len(self.vocab)}\")\n        print(f\"Sample vocabulary entries: {self.vocab[:10]}...{self.vocab[-10:]}\")\n    \n    def _compute_pair_freqs(self):\n        pair_freqs = defaultdict(int)\n        for word, freq in self.word_freqs.items():\n            split = self.splits[word]\n            if len(split) == 1:\n                continue\n            for i in range(len(split) - 1):\n                pair = (split[i], split[i + 1])\n                pair_freqs[pair] += freq\n        \n        return pair_freqs\n    \n    def _merge_pair(self, a: str, b: str):\n        new_splits = {}\n        for word, split in self.splits.items():\n            if len(split) == 1:\n                new_splits[word] = split\n                continue\n\n            new_split = []\n            i = 0\n            while i < len(split):\n                if i < len(split) - 1 and split[i] == a and split[i + 1] == b:\n                    new_split.append(a + b)\n                    i += 2\n                else:\n                    new_split.append(split[i])\n                    i += 1\n            new_splits[word] = new_split\n        return new_splits\n\n    def flatmap_tokens(self, nested_lists: List[List[List[str]]]) -> List[List[str]]:\n        print(f\"\\nFlattening nested token structure...\")\n        result = []\n\n        for recipe_idx, recipe_tokens in enumerate(nested_lists):\n            if recipe_idx % 100 == 0 and recipe_idx > 0:\n                print(f\"  - Flattened {recipe_idx} recipes\")\n                \n            flattened = []\n            for item_tokens in recipe_tokens:\n                flattened.extend(item_tokens)\n            \n            result.append(flattened)\n            \n        print(f\"Flattening complete. Converted {len(nested_lists)} nested lists to flat token lists.\")\n        return result\n\n    def tokenize(self, text: str) -> List[str]:\n        chars = [c for c in text]\n        \n        i = 0\n        merges_applied = 0\n        while i < len(chars) - 1:\n            pair = (chars[i], chars[i + 1])\n            if pair in self.merges:\n                chars = chars[:i] + [self.merges[pair]] + chars[i+2:]\n                merges_applied += 1\n            else:\n                i += 1\n        \n        return chars\n\n    def tokenize_list(self, text_list: List[str]) -> List[List[str]]:\n        return [self.tokenize(text) for text in text_list]\n\n    def fit(self, X: pd.DataFrame, y=None):\n        print(f\"\\n{'='*50}\")\n        print(f\"FITTING BPE TOKENIZER ON {len(X)} SAMPLES\")\n        print(f\"{'='*50}\")\n        \n        missing_cols = [col for col in [self.ingredients_col, self.steps_col] if col not in X.columns]\n        if missing_cols:\n            raise ValueError(f\"Columns {missing_cols} not found in the DataFrame\")\n\n        corpus = self._collect_corpus(X)\n        \n        self._train_bpe(corpus)\n\n        print(f\"\\nFit complete!\")\n        return self\n\n    def transform(self, X: pd.DataFrame) -> Tuple[List[List[str]], List[List[str]]]:\n        print(f\"\\n{'='*50}\")\n        print(f\"TRANSFORMING {len(X)} SAMPLES\")\n        print(f\"{'='*50}\")\n        \n        X = X.dropna(subset=[self.ingredients_col, self.steps_col])\n        print(f\"After dropping NaN values: {len(X)} samples remain\")\n        \n        ingredients_lists = []\n        steps_lists = []\n        ingredients_counter = defaultdict(int)\n\n        print(\"\\nExtracting ingredients and steps...\")\n        for idx, row in X.iterrows():\n            if idx % 100 == 0 and idx > 0:\n                print(f\"  - Processed {idx} rows\")\n                \n            ingredients = self._extract_ingredients(row)\n            steps = self._extract_steps(row)\n            \n            for ing in ingredients:\n                ingredients_counter[ing] += 1\n                \n            ingredients_lists.append(ingredients)\n            steps_lists.append(steps)\n\n        print(f\"\\nExtracted {len(ingredients_lists)} ingredient lists and {len(steps_lists)} step lists\")\n        \n        if self.drop_uncommon is not None:\n            print(f\"\\nFiltering uncommon ingredients (threshold: {self.drop_uncommon})...\")\n            total_ingredients = sum(len(ing_list) for ing_list in ingredients_lists)\n            print(f\"Total ingredients before filtering: {total_ingredients}\")\n            \n            uncommon_ingredients = {\n                ing for ing, count in ingredients_counter.items()\n                if count < self.drop_uncommon\n            }\n            \n            print(f\"Found {len(uncommon_ingredients)} uncommon ingredients to filter\")\n            print(f\"Examples of uncommon ingredients: {list(uncommon_ingredients)[:5]}\")\n    \n            filtered_ingredients_lists = []\n            filtered_steps_lists = []\n            for ingredients, steps in zip(ingredients_lists, steps_lists):\n                if not any(ing in uncommon_ingredients for ing in ingredients):\n                    filtered_ingredients_lists.append(ingredients)\n                    filtered_steps_lists.append(steps)\n    \n            print(f\"After filtering: {len(filtered_ingredients_lists)} samples remain\")\n            ingredients_lists = filtered_ingredients_lists\n            steps_lists = filtered_steps_lists\n\n        print(\"\\nTokenizing ingredients and steps...\")\n        tokenized_ingredients_lists = [self.tokenize_list(ingredients) for ingredients in ingredients_lists]\n        tokenized_steps_lists = [self.tokenize_list(steps) for steps in steps_lists]\n        \n        print(\"\\nFlattening token structures...\")\n        flat_ingredients_lists = self.flatmap_tokens(tokenized_ingredients_lists)\n        flat_steps_lists = self.flatmap_tokens(tokenized_steps_lists)\n        \n        print(f\"\\nFinal result: {len(flat_ingredients_lists)} ingredient token lists, {len(flat_steps_lists)} step token lists\")\n        if flat_ingredients_lists and flat_steps_lists:\n            print(f\"Example ingredients tokens (first recipe): {flat_ingredients_lists[0][:10]}...\")\n            print(f\"Example steps tokens (first recipe): {flat_steps_lists[0][:10]}...\")\n        \n        print(\"\\nTransform complete!\")\n        return flat_ingredients_lists, flat_steps_lists\n\n    def fit_transform(self, X: pd.DataFrame, y=None) -> Tuple[List[List[str]], List[List[str]]]:\n        print(\"\\nPerforming fit_transform...\")\n        return self.fit(X).transform(X)\n\n    def save_in_dir(self, directory: str):\n        self.save(self.get_savepath(directory))\n\n    def save(self, path: str):\n        print(f\"\\nSaving tokenizer to {path}...\")\n        tokenizer_data = {\n            'vocab': self.vocab,\n            'merges': self.merges,\n            'word_freqs': dict(self.word_freqs),\n            'splits': self.splits\n        }\n        with open(path, 'wb') as f:\n            pickle.dump(tokenizer_data, f)\n        print(\"Tokenizer saved successfully!\")\n    \n    def load(self, path: str):\n        print(f\"\\nLoading tokenizer from {path}...\")\n        with open(path, 'rb') as f:\n            tokenizer_data = pickle.load(f)\n        self.vocab = tokenizer_data['vocab']\n        self.merges = tokenizer_data['merges']\n        self.word_freqs = defaultdict(int)\n        self.word_freqs.update(tokenizer_data['word_freqs'])\n        self.splits = tokenizer_data['splits']\n        print(f\"Tokenizer loaded successfully!\")\n        print(f\"Vocabulary size: {len(self.vocab)}\")\n        print(f\"Number of merges: {len(self.merges)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:13:36.869307Z","iopub.execute_input":"2025-05-09T14:13:36.869795Z","iopub.status.idle":"2025-05-09T14:13:36.996947Z","shell.execute_reply.started":"2025-05-09T14:13:36.869773Z","shell.execute_reply":"2025-05-09T14:13:36.996239Z"}},"outputs":[],"execution_count":36},{"id":"bbfbe535-1aac-4c28-9087-4b29f2cb43ae","cell_type":"code","source":"sample = 20000\nbpe_generator_preprocessing = BPEGeneratorPreprocessing(vocab_size=80)\nbpe_generator_preprocessing.fit(data.iloc[:sample])\ningredients, steps = bpe_generator_preprocessing.transform(data)\ningredients_array = np.array(ingredients, dtype=object)\nsteps_array = np.array(steps, dtype=object)\ningredients_array = np.array(ingredients, dtype=object)\nsteps_array = np.array(steps, dtype=object)\nnp.savez_compressed(\"/kaggle/working/bpe_80_tokenized_data.npz\", ingredients=ingredients_array, steps=steps_array)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:15:15.667437Z","iopub.execute_input":"2025-05-09T14:15:15.668141Z","iopub.status.idle":"2025-05-09T14:18:11.734147Z","shell.execute_reply.started":"2025-05-09T14:15:15.668111Z","shell.execute_reply":"2025-05-09T14:18:11.733372Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"\n==================================================\nFITTING BPE TOKENIZER ON 20000 SAMPLES\n==================================================\n\nCollecting corpus from 20000 rows...\n  - Processed 100 rows, corpus size: 1534\n  - Processed 200 rows, corpus size: 3248\n  - Processed 300 rows, corpus size: 4758\n  - Processed 400 rows, corpus size: 6269\n  - Processed 500 rows, corpus size: 7839\n  - Processed 600 rows, corpus size: 9543\n  - Processed 700 rows, corpus size: 11095\n  - Processed 800 rows, corpus size: 12896\n  - Processed 900 rows, corpus size: 14504\n  - Processed 1000 rows, corpus size: 16079\n  - Processed 1100 rows, corpus size: 17720\n  - No steps found (NaN value)\n  - Processed 1200 rows, corpus size: 19370\n  - Processed 1300 rows, corpus size: 20923\n  - Processed 1400 rows, corpus size: 22663\n  - Processed 1500 rows, corpus size: 24215\n  - Processed 1600 rows, corpus size: 25995\n  - Processed 1700 rows, corpus size: 27748\n  - Processed 1800 rows, corpus size: 29324\n  - Processed 1900 rows, corpus size: 30848\n  - Processed 2000 rows, corpus size: 32427\n  - Processed 2100 rows, corpus size: 33983\n  - Processed 2200 rows, corpus size: 35408\n  - Processed 2300 rows, corpus size: 36964\n  - Processed 2400 rows, corpus size: 38478\n  - Processed 2500 rows, corpus size: 40170\n  - Processed 2600 rows, corpus size: 41794\n  - Processed 2700 rows, corpus size: 43507\n  - Processed 2800 rows, corpus size: 45087\n  - Processed 2900 rows, corpus size: 46950\n  - Processed 3000 rows, corpus size: 48836\n  - Processed 3100 rows, corpus size: 50495\n  - Processed 3200 rows, corpus size: 52288\n  - Processed 3300 rows, corpus size: 54057\n  - Processed 3400 rows, corpus size: 55899\n  - Processed 3500 rows, corpus size: 57649\n  - Processed 3600 rows, corpus size: 59189\n  - Processed 3700 rows, corpus size: 60647\n  - Processed 3800 rows, corpus size: 62523\n  - Processed 3900 rows, corpus size: 64520\n  - Processed 4000 rows, corpus size: 66092\n  - Processed 4100 rows, corpus size: 67552\n  - Processed 4200 rows, corpus size: 69334\n  - Processed 4300 rows, corpus size: 70873\n  - Processed 4400 rows, corpus size: 72452\n  - Processed 4500 rows, corpus size: 73980\n  - Processed 4600 rows, corpus size: 75558\n  - Processed 4700 rows, corpus size: 77161\n  - Processed 4800 rows, corpus size: 78606\n  - Processed 4900 rows, corpus size: 80194\n  - Processed 5000 rows, corpus size: 81780\n  - Processed 5100 rows, corpus size: 83497\n  - Processed 5200 rows, corpus size: 85112\n  - Processed 5300 rows, corpus size: 86729\n  - Processed 5400 rows, corpus size: 88554\n  - Processed 5500 rows, corpus size: 90305\n  - Processed 5600 rows, corpus size: 91896\n  - Processed 5700 rows, corpus size: 93378\n  - Processed 5800 rows, corpus size: 94789\n  - Processed 5900 rows, corpus size: 96519\n  - Processed 6000 rows, corpus size: 98141\n  - Processed 6100 rows, corpus size: 99879\n  - Processed 6200 rows, corpus size: 101431\n  - Processed 6300 rows, corpus size: 102816\n  - Processed 6400 rows, corpus size: 104411\n  - Processed 6500 rows, corpus size: 106045\n  - Processed 6600 rows, corpus size: 107657\n  - Processed 6700 rows, corpus size: 109354\n  - Processed 6800 rows, corpus size: 111374\n  - Processed 6900 rows, corpus size: 113542\n  - Processed 7000 rows, corpus size: 115599\n  - Processed 7100 rows, corpus size: 117761\n  - Processed 7200 rows, corpus size: 119634\n  - Processed 7300 rows, corpus size: 121269\n  - Processed 7400 rows, corpus size: 122926\n  - Processed 7500 rows, corpus size: 124360\n  - Processed 7600 rows, corpus size: 125983\n  - Processed 7700 rows, corpus size: 127602\n  - Processed 7800 rows, corpus size: 129282\n  - Processed 7900 rows, corpus size: 130949\n  - Processed 8000 rows, corpus size: 132653\n  - Processed 8100 rows, corpus size: 134391\n  - Processed 8200 rows, corpus size: 136090\n  - Processed 8300 rows, corpus size: 138121\n  - Processed 8400 rows, corpus size: 139812\n  - Processed 8500 rows, corpus size: 141404\n  - Processed 8600 rows, corpus size: 142998\n  - Processed 8700 rows, corpus size: 144365\n  - Processed 8800 rows, corpus size: 145982\n  - Processed 8900 rows, corpus size: 147469\n  - Processed 9000 rows, corpus size: 149327\n  - Processed 9100 rows, corpus size: 150990\n  - Processed 9200 rows, corpus size: 152747\n  - Processed 9300 rows, corpus size: 154357\n  - Processed 9400 rows, corpus size: 156224\n  - Processed 9500 rows, corpus size: 157976\n  - Processed 9600 rows, corpus size: 159568\n  - Processed 9700 rows, corpus size: 161129\n  - Processed 9800 rows, corpus size: 162667\n  - Processed 9900 rows, corpus size: 164128\n  - Processed 10000 rows, corpus size: 165656\n  - Processed 10100 rows, corpus size: 167138\n  - Processed 10200 rows, corpus size: 168741\n  - Processed 10300 rows, corpus size: 170307\n  - Processed 10400 rows, corpus size: 171884\n  - Processed 10500 rows, corpus size: 173601\n  - Processed 10600 rows, corpus size: 175241\n  - Processed 10700 rows, corpus size: 177181\n  - Processed 10800 rows, corpus size: 178796\n  - Processed 10900 rows, corpus size: 180271\n  - Processed 11000 rows, corpus size: 181988\n  - Processed 11100 rows, corpus size: 183644\n  - Processed 11200 rows, corpus size: 185044\n  - Processed 11300 rows, corpus size: 186534\n  - Processed 11400 rows, corpus size: 188272\n  - Processed 11500 rows, corpus size: 190092\n  - Processed 11600 rows, corpus size: 191889\n  - Processed 11700 rows, corpus size: 193680\n  - Processed 11800 rows, corpus size: 195467\n  - Processed 11900 rows, corpus size: 196992\n  - Processed 12000 rows, corpus size: 198402\n  - Processed 12100 rows, corpus size: 200189\n  - Processed 12200 rows, corpus size: 201816\n  - Processed 12300 rows, corpus size: 203249\n  - Processed 12400 rows, corpus size: 204894\n  - Processed 12500 rows, corpus size: 206696\n  - Processed 12600 rows, corpus size: 208585\n  - Processed 12700 rows, corpus size: 210326\n  - Processed 12800 rows, corpus size: 212008\n  - Processed 12900 rows, corpus size: 213756\n  - Processed 13000 rows, corpus size: 215574\n  - Processed 13100 rows, corpus size: 217259\n  - Processed 13200 rows, corpus size: 218882\n  - Processed 13300 rows, corpus size: 220353\n  - Processed 13400 rows, corpus size: 222179\n  - Processed 13500 rows, corpus size: 223812\n  - Processed 13600 rows, corpus size: 225297\n  - Processed 13700 rows, corpus size: 226804\n  - Processed 13800 rows, corpus size: 228448\n  - Processed 13900 rows, corpus size: 230068\n  - Processed 14000 rows, corpus size: 231568\n  - Processed 14100 rows, corpus size: 232988\n  - Processed 14200 rows, corpus size: 234569\n  - Processed 14300 rows, corpus size: 235963\n  - Processed 14400 rows, corpus size: 237717\n  - Processed 14500 rows, corpus size: 239757\n  - Processed 14600 rows, corpus size: 241756\n  - Processed 14700 rows, corpus size: 243764\n  - Processed 14800 rows, corpus size: 245815\n  - Processed 14900 rows, corpus size: 247808\n  - Processed 15000 rows, corpus size: 249764\n  - Processed 15100 rows, corpus size: 251773\n  - Processed 15200 rows, corpus size: 253912\n  - Processed 15300 rows, corpus size: 255968\n  - Processed 15400 rows, corpus size: 258114\n  - Processed 15500 rows, corpus size: 260086\n  - Processed 15600 rows, corpus size: 262066\n  - Processed 15700 rows, corpus size: 264119\n  - Processed 15800 rows, corpus size: 266122\n  - Processed 15900 rows, corpus size: 268318\n  - Processed 16000 rows, corpus size: 270513\n  - Processed 16100 rows, corpus size: 272483\n  - Processed 16200 rows, corpus size: 274466\n  - Processed 16300 rows, corpus size: 276564\n  - Processed 16400 rows, corpus size: 278286\n  - Processed 16500 rows, corpus size: 280162\n  - Processed 16600 rows, corpus size: 281718\n  - Processed 16700 rows, corpus size: 283387\n  - Processed 16800 rows, corpus size: 285053\n  - Processed 16900 rows, corpus size: 286979\n  - Processed 17000 rows, corpus size: 288668\n  - Processed 17100 rows, corpus size: 290470\n  - Processed 17200 rows, corpus size: 292344\n  - Processed 17300 rows, corpus size: 294004\n  - Processed 17400 rows, corpus size: 295673\n  - Processed 17500 rows, corpus size: 297350\n  - Processed 17600 rows, corpus size: 298879\n  - Processed 17700 rows, corpus size: 300346\n  - Processed 17800 rows, corpus size: 301984\n  - Processed 17900 rows, corpus size: 303668\n  - Processed 18000 rows, corpus size: 305410\n  - Processed 18100 rows, corpus size: 307024\n  - Processed 18200 rows, corpus size: 308723\n  - Processed 18300 rows, corpus size: 310344\n  - Processed 18400 rows, corpus size: 311826\n  - Processed 18500 rows, corpus size: 313358\n  - Processed 18600 rows, corpus size: 314985\n  - Processed 18700 rows, corpus size: 316652\n  - Processed 18800 rows, corpus size: 318519\n  - Processed 18900 rows, corpus size: 319992\n  - Processed 19000 rows, corpus size: 321868\n  - Processed 19100 rows, corpus size: 323502\n  - Processed 19200 rows, corpus size: 325013\n  - Processed 19300 rows, corpus size: 326805\n  - Processed 19400 rows, corpus size: 328446\n  - Processed 19500 rows, corpus size: 330228\n  - Processed 19600 rows, corpus size: 331889\n  - Processed 19700 rows, corpus size: 333611\n  - Processed 19800 rows, corpus size: 335137\n  - Processed 19900 rows, corpus size: 336795\nFinal corpus size: 338452 tokens\nSample corpus entries: ['winter squash', 'mexican seasoning', 'mixed spice', 'honey', 'but']\n\nTraining BPE tokenizer...\nBuilding initial word frequencies...\nUnique words in corpus: 114825\nTop 5 most common words: [('salt', 9465), ('on', 5729), ('but', 4944), ('sug', 4758), ('eg', 4709)]\n\nBuilding initial alphabet...\nAlphabet size: 46\nInitial vocabulary size: 49\n\nInitializing splits (words into characters)...\n\nStarting BPE merge operations...\n\nIteration 1: Current vocab size = 49\n  Computing pair frequencies...\n  Best pair: ('e', ' ') with frequency 281357\n  Merging pair 'e' + ' '...\n\nIteration 2: Current vocab size = 50\n  Computing pair frequencies...\n  Best pair: ('n', ' ') with frequency 243990\n  Merging pair 'n' + ' '...\n\nIteration 3: Current vocab size = 51\n  Computing pair frequencies...\n  Best pair: ('r', ' ') with frequency 221376\n  Merging pair 'r' + ' '...\n\nIteration 4: Current vocab size = 52\n  Computing pair frequencies...\n  Best pair: ('d', ' ') with frequency 221292\n  Merging pair 'd' + ' '...\n\nIteration 5: Current vocab size = 53\n  Computing pair frequencies...\n  Best pair: ('t', ' ') with frequency 204360\n  Merging pair 't' + ' '...\n\nIteration 6: Current vocab size = 54\n  Computing pair frequencies...\n  Best pair: ('t', 'h') with frequency 201379\n  Merging pair 't' + 'h'...\n\nIteration 7: Current vocab size = 55\n  Computing pair frequencies...\n  Best pair: ('a', 'n') with frequency 156773\n  Merging pair 'a' + 'n'...\n\nIteration 8: Current vocab size = 56\n  Computing pair frequencies...\n  Best pair: ('l', ' ') with frequency 135069\n  Merging pair 'l' + ' '...\n\nIteration 9: Current vocab size = 57\n  Computing pair frequencies...\n  Best pair: ('t', 'o') with frequency 127610\n  Merging pair 't' + 'o'...\n\nIteration 10: Current vocab size = 58\n  Computing pair frequencies...\n  Best pair: ('i', 'n') with frequency 124541\n  Merging pair 'i' + 'n'...\n\nIteration 11: Current vocab size = 59\n  Computing pair frequencies...\n  Best pair: ('an', 'd ') with frequency 114788\n  Merging pair 'an' + 'd '...\n\nIteration 12: Current vocab size = 60\n  Computing pair frequencies...\n  Best pair: ('r', 'e') with frequency 111152\n  Merging pair 'r' + 'e'...\n\nIteration 13: Current vocab size = 61\n  Computing pair frequencies...\n  Best pair: ('th', 'e ') with frequency 110309\n  Merging pair 'th' + 'e '...\n\nIteration 14: Current vocab size = 62\n  Computing pair frequencies...\n  Best pair: ('e', 'r ') with frequency 99616\n  Merging pair 'e' + 'r '...\n\nIteration 15: Current vocab size = 63\n  Computing pair frequencies...\n  Best pair: ('c', 'o') with frequency 94329\n  Merging pair 'c' + 'o'...\n\nIteration 16: Current vocab size = 64\n  Computing pair frequencies...\n  Best pair: ('i', 'n ') with frequency 86968\n  Merging pair 'i' + 'n '...\n\nIteration 17: Current vocab size = 65\n  Computing pair frequencies...\n  Best pair: ('a', 'r') with frequency 81290\n  Merging pair 'a' + 'r'...\n\nIteration 18: Current vocab size = 66\n  Computing pair frequencies...\n  Best pair: ('c', 'h') with frequency 80948\n  Merging pair 'c' + 'h'...\n\nIteration 19: Current vocab size = 67\n  Computing pair frequencies...\n  Best pair: (' ', 's') with frequency 78986\n  Merging pair ' ' + 's'...\n\nIteration 20: Current vocab size = 68\n  Computing pair frequencies...\n  Best pair: ('o', 'u') with frequency 76952\n  Merging pair 'o' + 'u'...\n\nIteration 21: Current vocab size = 69\n  Computing pair frequencies...\n  Best pair: ('e', 'a') with frequency 76793\n  Merging pair 'e' + 'a'...\n\nIteration 22: Current vocab size = 70\n  Computing pair frequencies...\n  Best pair: ('l', 'i') with frequency 73552\n  Merging pair 'l' + 'i'...\n\nIteration 23: Current vocab size = 71\n  Computing pair frequencies...\n  Best pair: ('t', 'i') with frequency 71083\n  Merging pair 't' + 'i'...\n\nIteration 24: Current vocab size = 72\n  Computing pair frequencies...\n  Best pair: ('to', ' ') with frequency 70506\n  Merging pair 'to' + ' '...\n\nIteration 25: Current vocab size = 73\n  Computing pair frequencies...\n  Best pair: ('e', 'r') with frequency 67491\n  Merging pair 'e' + 'r'...\n\nIteration 26: Current vocab size = 74\n  Computing pair frequencies...\n  Best pair: ('a', ' ') with frequency 63250\n  Merging pair 'a' + ' '...\n\nIteration 27: Current vocab size = 75\n  Computing pair frequencies...\n  Best pair: ('a', 'l') with frequency 62928\n  Merging pair 'a' + 'l'...\n\nIteration 28: Current vocab size = 76\n  Computing pair frequencies...\n  Best pair: ('u', 't') with frequency 56457\n  Merging pair 'u' + 't'...\n\nIteration 29: Current vocab size = 77\n  Computing pair frequencies...\n  Best pair: (' ', 'm') with frequency 55210\n  Merging pair ' ' + 'm'...\n\nIteration 30: Current vocab size = 78\n  Computing pair frequencies...\n  Best pair: ('e', 'n ') with frequency 54758\n  Merging pair 'e' + 'n '...\n\nIteration 31: Current vocab size = 79\n  Computing pair frequencies...\n  Best pair: ('r', 'o') with frequency 53656\n  Merging pair 'r' + 'o'...\n\nBPE training complete. Final vocabulary size: 80\nSample vocabulary entries: ['<s>', '</s>', '<RECIPE>', ' ', '!', '\"', '%', '&', \"'\", '*']...['li', 'ti', 'to ', 'er', 'a ', 'al', 'ut', ' m', 'en ', 'ro']\n\nFit complete!\n\n==================================================\nTRANSFORMING 80601 SAMPLES\n==================================================\nAfter dropping NaN values: 80600 samples remain\n\nExtracting ingredients and steps...\n  - Processed 100 rows\n  - Processed 200 rows\n  - Processed 300 rows\n  - Processed 400 rows\n  - Processed 500 rows\n  - Processed 600 rows\n  - Processed 700 rows\n  - Processed 800 rows\n  - Processed 900 rows\n  - Processed 1000 rows\n  - Processed 1100 rows\n  - Processed 1200 rows\n  - Processed 1300 rows\n  - Processed 1400 rows\n  - Processed 1500 rows\n  - Processed 1600 rows\n  - Processed 1700 rows\n  - Processed 1800 rows\n  - Processed 1900 rows\n  - Processed 2000 rows\n  - Processed 2100 rows\n  - Processed 2200 rows\n  - Processed 2300 rows\n  - Processed 2400 rows\n  - Processed 2500 rows\n  - Processed 2600 rows\n  - Processed 2700 rows\n  - Processed 2800 rows\n  - Processed 2900 rows\n  - Processed 3000 rows\n  - Processed 3100 rows\n  - Processed 3200 rows\n  - Processed 3300 rows\n  - Processed 3400 rows\n  - Processed 3500 rows\n  - Processed 3600 rows\n  - Processed 3700 rows\n  - Processed 3800 rows\n  - Processed 3900 rows\n  - Processed 4000 rows\n  - Processed 4100 rows\n  - Processed 4200 rows\n  - Processed 4300 rows\n  - Processed 4400 rows\n  - Processed 4500 rows\n  - Processed 4600 rows\n  - Processed 4700 rows\n  - Processed 4800 rows\n  - Processed 4900 rows\n  - Processed 5000 rows\n  - Processed 5100 rows\n  - Processed 5200 rows\n  - Processed 5300 rows\n  - Processed 5400 rows\n  - Processed 5500 rows\n  - Processed 5600 rows\n  - Processed 5700 rows\n  - Processed 5800 rows\n  - Processed 5900 rows\n  - Processed 6000 rows\n  - Processed 6100 rows\n  - Processed 6200 rows\n  - Processed 6300 rows\n  - Processed 6400 rows\n  - Processed 6500 rows\n  - Processed 6600 rows\n  - Processed 6700 rows\n  - Processed 6800 rows\n  - Processed 6900 rows\n  - Processed 7000 rows\n  - Processed 7100 rows\n  - Processed 7200 rows\n  - Processed 7300 rows\n  - Processed 7400 rows\n  - Processed 7500 rows\n  - Processed 7600 rows\n  - Processed 7700 rows\n  - Processed 7800 rows\n  - Processed 7900 rows\n  - Processed 8000 rows\n  - Processed 8100 rows\n  - Processed 8200 rows\n  - Processed 8300 rows\n  - Processed 8400 rows\n  - Processed 8500 rows\n  - Processed 8600 rows\n  - Processed 8700 rows\n  - Processed 8800 rows\n  - Processed 8900 rows\n  - Processed 9000 rows\n  - Processed 9100 rows\n  - Processed 9200 rows\n  - Processed 9300 rows\n  - Processed 9400 rows\n  - Processed 9500 rows\n  - Processed 9600 rows\n  - Processed 9700 rows\n  - Processed 9800 rows\n  - Processed 9900 rows\n  - Processed 10000 rows\n  - Processed 10100 rows\n  - Processed 10200 rows\n  - Processed 10300 rows\n  - Processed 10400 rows\n  - Processed 10500 rows\n  - Processed 10600 rows\n  - Processed 10700 rows\n  - Processed 10800 rows\n  - Processed 10900 rows\n  - Processed 11000 rows\n  - Processed 11100 rows\n  - Processed 11200 rows\n  - Processed 11300 rows\n  - Processed 11400 rows\n  - Processed 11500 rows\n  - Processed 11600 rows\n  - Processed 11700 rows\n  - Processed 11800 rows\n  - Processed 11900 rows\n  - Processed 12000 rows\n  - Processed 12100 rows\n  - Processed 12200 rows\n  - Processed 12300 rows\n  - Processed 12400 rows\n  - Processed 12500 rows\n  - Processed 12600 rows\n  - Processed 12700 rows\n  - Processed 12800 rows\n  - Processed 12900 rows\n  - Processed 13000 rows\n  - Processed 13100 rows\n  - Processed 13200 rows\n  - Processed 13300 rows\n  - Processed 13400 rows\n  - Processed 13500 rows\n  - Processed 13600 rows\n  - Processed 13700 rows\n  - Processed 13800 rows\n  - Processed 13900 rows\n  - Processed 14000 rows\n  - Processed 14100 rows\n  - Processed 14200 rows\n  - Processed 14300 rows\n  - Processed 14400 rows\n  - Processed 14500 rows\n  - Processed 14600 rows\n  - Processed 14700 rows\n  - Processed 14800 rows\n  - Processed 14900 rows\n  - Processed 15000 rows\n  - Processed 15100 rows\n  - Processed 15200 rows\n  - Processed 15300 rows\n  - Processed 15400 rows\n  - Processed 15500 rows\n  - Processed 15600 rows\n  - Processed 15700 rows\n  - Processed 15800 rows\n  - Processed 15900 rows\n  - Processed 16000 rows\n  - Processed 16100 rows\n  - Processed 16200 rows\n  - Processed 16300 rows\n  - Processed 16400 rows\n  - Processed 16500 rows\n  - Processed 16600 rows\n  - Processed 16700 rows\n  - Processed 16800 rows\n  - Processed 16900 rows\n  - Processed 17000 rows\n  - Processed 17100 rows\n  - Processed 17200 rows\n  - Processed 17300 rows\n  - Processed 17400 rows\n  - Processed 17500 rows\n  - Processed 17600 rows\n  - Processed 17700 rows\n  - Processed 17800 rows\n  - Processed 17900 rows\n  - Processed 18000 rows\n  - Processed 18100 rows\n  - Processed 18200 rows\n  - Processed 18300 rows\n  - Processed 18400 rows\n  - Processed 18500 rows\n  - Processed 18600 rows\n  - Processed 18700 rows\n  - Processed 18800 rows\n  - Processed 18900 rows\n  - Processed 19000 rows\n  - Processed 19100 rows\n  - Processed 19200 rows\n  - Processed 19300 rows\n  - Processed 19400 rows\n  - Processed 19500 rows\n  - Processed 19600 rows\n  - Processed 19700 rows\n  - Processed 19800 rows\n  - Processed 19900 rows\n  - Processed 20000 rows\n  - Processed 20100 rows\n  - Processed 20200 rows\n  - Processed 20300 rows\n  - Processed 20400 rows\n  - Processed 20500 rows\n  - Processed 20600 rows\n  - Processed 20700 rows\n  - Processed 20800 rows\n  - Processed 20900 rows\n  - Processed 21000 rows\n  - Processed 21100 rows\n  - Processed 21200 rows\n  - Processed 21300 rows\n  - Processed 21400 rows\n  - Processed 21500 rows\n  - Processed 21600 rows\n  - Processed 21700 rows\n  - Processed 21800 rows\n  - Processed 21900 rows\n  - Processed 22000 rows\n  - Processed 22100 rows\n  - Processed 22200 rows\n  - Processed 22300 rows\n  - Processed 22400 rows\n  - Processed 22500 rows\n  - Processed 22600 rows\n  - Processed 22700 rows\n  - Processed 22800 rows\n  - Processed 22900 rows\n  - Processed 23000 rows\n  - Processed 23100 rows\n  - Processed 23200 rows\n  - Processed 23300 rows\n  - Processed 23400 rows\n  - Processed 23500 rows\n  - Processed 23600 rows\n  - Processed 23700 rows\n  - Processed 23800 rows\n  - Processed 23900 rows\n  - Processed 24000 rows\n  - Processed 24100 rows\n  - Processed 24200 rows\n  - Processed 24300 rows\n  - Processed 24400 rows\n  - Processed 24500 rows\n  - Processed 24600 rows\n  - Processed 24700 rows\n  - Processed 24800 rows\n  - Processed 24900 rows\n  - Processed 25000 rows\n  - Processed 25100 rows\n  - Processed 25200 rows\n  - Processed 25300 rows\n  - Processed 25400 rows\n  - Processed 25500 rows\n  - Processed 25600 rows\n  - Processed 25700 rows\n  - Processed 25800 rows\n  - Processed 25900 rows\n  - Processed 26000 rows\n  - Processed 26100 rows\n  - Processed 26200 rows\n  - Processed 26300 rows\n  - Processed 26400 rows\n  - Processed 26500 rows\n  - Processed 26600 rows\n  - Processed 26700 rows\n  - Processed 26800 rows\n  - Processed 26900 rows\n  - Processed 27000 rows\n  - Processed 27100 rows\n  - Processed 27200 rows\n  - Processed 27300 rows\n  - Processed 27400 rows\n  - Processed 27500 rows\n  - Processed 27600 rows\n  - Processed 27700 rows\n  - Processed 27800 rows\n  - Processed 27900 rows\n  - Processed 28000 rows\n  - Processed 28100 rows\n  - Processed 28200 rows\n  - Processed 28300 rows\n  - Processed 28400 rows\n  - Processed 28500 rows\n  - Processed 28600 rows\n  - Processed 28700 rows\n  - Processed 28800 rows\n  - Processed 28900 rows\n  - Processed 29000 rows\n  - Processed 29100 rows\n  - Processed 29200 rows\n  - Processed 29300 rows\n  - Processed 29400 rows\n  - Processed 29500 rows\n  - Processed 29600 rows\n  - Processed 29700 rows\n  - Processed 29800 rows\n  - Processed 29900 rows\n  - Processed 30000 rows\n  - Processed 30100 rows\n  - Processed 30200 rows\n  - Processed 30300 rows\n  - Processed 30400 rows\n  - Processed 30500 rows\n  - Processed 30600 rows\n  - Processed 30700 rows\n  - Processed 30800 rows\n  - Processed 30900 rows\n  - Processed 31000 rows\n  - Processed 31100 rows\n  - Processed 31200 rows\n  - Processed 31300 rows\n  - Processed 31400 rows\n  - Processed 31500 rows\n  - Processed 31600 rows\n  - Processed 31700 rows\n  - Processed 31800 rows\n  - Processed 31900 rows\n  - Processed 32000 rows\n  - Processed 32100 rows\n  - Processed 32200 rows\n  - Processed 32300 rows\n  - Processed 32400 rows\n  - Processed 32500 rows\n  - Processed 32600 rows\n  - Processed 32700 rows\n  - Processed 32800 rows\n  - Processed 32900 rows\n  - Processed 33000 rows\n  - Processed 33100 rows\n  - Processed 33200 rows\n  - Processed 33300 rows\n  - Processed 33400 rows\n  - Processed 33500 rows\n  - Processed 33600 rows\n  - Processed 33700 rows\n  - Processed 33800 rows\n  - Processed 33900 rows\n  - Processed 34000 rows\n  - Processed 34100 rows\n  - Processed 34200 rows\n  - Processed 34300 rows\n  - Processed 34400 rows\n  - Processed 34500 rows\n  - Processed 34600 rows\n  - Processed 34700 rows\n  - Processed 34800 rows\n  - Processed 34900 rows\n  - Processed 35000 rows\n  - Processed 35100 rows\n  - Processed 35200 rows\n  - Processed 35300 rows\n  - Processed 35400 rows\n  - Processed 35500 rows\n  - Processed 35600 rows\n  - Processed 35700 rows\n  - Processed 35800 rows\n  - Processed 35900 rows\n  - Processed 36000 rows\n  - Processed 36100 rows\n  - Processed 36200 rows\n  - Processed 36300 rows\n  - Processed 36400 rows\n  - Processed 36500 rows\n  - Processed 36600 rows\n  - Processed 36700 rows\n  - Processed 36800 rows\n  - Processed 36900 rows\n  - Processed 37000 rows\n  - Processed 37100 rows\n  - Processed 37200 rows\n  - Processed 37300 rows\n  - Processed 37400 rows\n  - Processed 37500 rows\n  - Processed 37600 rows\n  - Processed 37700 rows\n  - Processed 37800 rows\n  - Processed 37900 rows\n  - Processed 38000 rows\n  - Processed 38100 rows\n  - Processed 38200 rows\n  - Processed 38300 rows\n  - Processed 38400 rows\n  - Processed 38500 rows\n  - Processed 38600 rows\n  - Processed 38700 rows\n  - Processed 38800 rows\n  - Processed 38900 rows\n  - Processed 39000 rows\n  - Processed 39100 rows\n  - Processed 39200 rows\n  - Processed 39300 rows\n  - Processed 39400 rows\n  - Processed 39500 rows\n  - Processed 39600 rows\n  - Processed 39700 rows\n  - Processed 39800 rows\n  - Processed 39900 rows\n  - Processed 40000 rows\n  - Processed 40100 rows\n  - Processed 40200 rows\n  - Processed 40300 rows\n  - Processed 40400 rows\n  - Processed 40500 rows\n  - Processed 40600 rows\n  - Processed 40700 rows\n  - Processed 40800 rows\n  - Processed 40900 rows\n  - Processed 41000 rows\n  - Processed 41100 rows\n  - Processed 41200 rows\n  - Processed 41300 rows\n  - Processed 41400 rows\n  - Processed 41500 rows\n  - Processed 41600 rows\n  - Processed 41700 rows\n  - Processed 41800 rows\n  - Processed 41900 rows\n  - Processed 42000 rows\n  - Processed 42100 rows\n  - Processed 42200 rows\n  - Processed 42300 rows\n  - Processed 42400 rows\n  - Processed 42500 rows\n  - Processed 42600 rows\n  - Processed 42700 rows\n  - Processed 42800 rows\n  - Processed 42900 rows\n  - Processed 43000 rows\n  - Processed 43100 rows\n  - Processed 43200 rows\n  - Processed 43300 rows\n  - Processed 43400 rows\n  - Processed 43500 rows\n  - Processed 43600 rows\n  - Processed 43700 rows\n  - Processed 43800 rows\n  - Processed 43900 rows\n  - Processed 44000 rows\n  - Processed 44100 rows\n  - Processed 44200 rows\n  - Processed 44300 rows\n  - Processed 44400 rows\n  - Processed 44500 rows\n  - Processed 44600 rows\n  - Processed 44700 rows\n  - Processed 44800 rows\n  - Processed 44900 rows\n  - Processed 45000 rows\n  - Processed 45100 rows\n  - Processed 45200 rows\n  - Processed 45300 rows\n  - Processed 45400 rows\n  - Processed 45500 rows\n  - Processed 45600 rows\n  - Processed 45700 rows\n  - Processed 45800 rows\n  - Processed 45900 rows\n  - Processed 46000 rows\n  - Processed 46100 rows\n  - Processed 46200 rows\n  - Processed 46300 rows\n  - Processed 46400 rows\n  - Processed 46500 rows\n  - Processed 46600 rows\n  - Processed 46700 rows\n  - Processed 46800 rows\n  - Processed 46900 rows\n  - Processed 47000 rows\n  - Processed 47100 rows\n  - Processed 47200 rows\n  - Processed 47300 rows\n  - Processed 47400 rows\n  - Processed 47500 rows\n  - Processed 47600 rows\n  - Processed 47700 rows\n  - Processed 47800 rows\n  - Processed 47900 rows\n  - Processed 48000 rows\n  - Processed 48100 rows\n  - Processed 48200 rows\n  - Processed 48300 rows\n  - Processed 48400 rows\n  - Processed 48500 rows\n  - Processed 48600 rows\n  - Processed 48700 rows\n  - Processed 48800 rows\n  - Processed 48900 rows\n  - Processed 49000 rows\n  - Processed 49100 rows\n  - Processed 49200 rows\n  - Processed 49300 rows\n  - Processed 49400 rows\n  - Processed 49500 rows\n  - Processed 49600 rows\n  - Processed 49700 rows\n  - Processed 49800 rows\n  - Processed 49900 rows\n  - Processed 50000 rows\n  - Processed 50100 rows\n  - Processed 50200 rows\n  - Processed 50300 rows\n  - Processed 50400 rows\n  - Processed 50500 rows\n  - Processed 50600 rows\n  - Processed 50700 rows\n  - Processed 50800 rows\n  - Processed 50900 rows\n  - Processed 51000 rows\n  - Processed 51100 rows\n  - Processed 51200 rows\n  - Processed 51300 rows\n  - Processed 51400 rows\n  - Processed 51500 rows\n  - Processed 51600 rows\n  - Processed 51700 rows\n  - Processed 51800 rows\n  - Processed 51900 rows\n  - Processed 52000 rows\n  - Processed 52100 rows\n  - Processed 52200 rows\n  - Processed 52300 rows\n  - Processed 52400 rows\n  - Processed 52500 rows\n  - Processed 52600 rows\n  - Processed 52700 rows\n  - Processed 52800 rows\n  - Processed 52900 rows\n  - Processed 53000 rows\n  - Processed 53100 rows\n  - Processed 53200 rows\n  - Processed 53300 rows\n  - Processed 53400 rows\n  - Processed 53500 rows\n  - Processed 53600 rows\n  - Processed 53700 rows\n  - Processed 53800 rows\n  - Processed 53900 rows\n  - Processed 54000 rows\n  - Processed 54100 rows\n  - Processed 54200 rows\n  - Processed 54300 rows\n  - Processed 54400 rows\n  - Processed 54500 rows\n  - Processed 54600 rows\n  - Processed 54700 rows\n  - Processed 54800 rows\n  - Processed 54900 rows\n  - Processed 55000 rows\n  - Processed 55100 rows\n  - Processed 55200 rows\n  - Processed 55300 rows\n  - Processed 55400 rows\n  - Processed 55500 rows\n  - Processed 55600 rows\n  - Processed 55700 rows\n  - Processed 55800 rows\n  - Processed 55900 rows\n  - Processed 56000 rows\n  - Processed 56100 rows\n  - Processed 56200 rows\n  - Processed 56300 rows\n  - Processed 56400 rows\n  - Processed 56500 rows\n  - Processed 56600 rows\n  - Processed 56700 rows\n  - Processed 56800 rows\n  - Processed 56900 rows\n  - Processed 57000 rows\n  - Processed 57100 rows\n  - Processed 57200 rows\n  - Processed 57300 rows\n  - Processed 57400 rows\n  - Processed 57500 rows\n  - Processed 57600 rows\n  - Processed 57700 rows\n  - Processed 57800 rows\n  - Processed 57900 rows\n  - Processed 58000 rows\n  - Processed 58100 rows\n  - Processed 58200 rows\n  - Processed 58300 rows\n  - Processed 58400 rows\n  - Processed 58500 rows\n  - Processed 58600 rows\n  - Processed 58700 rows\n  - Processed 58800 rows\n  - Processed 58900 rows\n  - Processed 59000 rows\n  - Processed 59100 rows\n  - Processed 59200 rows\n  - Processed 59300 rows\n  - Processed 59400 rows\n  - Processed 59500 rows\n  - Processed 59600 rows\n  - Processed 59700 rows\n  - Processed 59800 rows\n  - Processed 59900 rows\n  - Processed 60000 rows\n  - Processed 60100 rows\n  - Processed 60200 rows\n  - Processed 60300 rows\n  - Processed 60400 rows\n  - Processed 60500 rows\n  - Processed 60600 rows\n  - Processed 60700 rows\n  - Processed 60800 rows\n  - Processed 60900 rows\n  - Processed 61000 rows\n  - Processed 61100 rows\n  - Processed 61200 rows\n  - Processed 61300 rows\n  - Processed 61400 rows\n  - Processed 61500 rows\n  - Processed 61600 rows\n  - Processed 61700 rows\n  - Processed 61800 rows\n  - Processed 61900 rows\n  - Processed 62000 rows\n  - Processed 62100 rows\n  - Processed 62200 rows\n  - Processed 62300 rows\n  - Processed 62400 rows\n  - Processed 62500 rows\n  - Processed 62600 rows\n  - Processed 62700 rows\n  - Processed 62800 rows\n  - Processed 62900 rows\n  - Processed 63000 rows\n  - Processed 63100 rows\n  - Processed 63200 rows\n  - Processed 63300 rows\n  - Processed 63400 rows\n  - Processed 63500 rows\n  - Processed 63600 rows\n  - Processed 63700 rows\n  - Processed 63800 rows\n  - Processed 63900 rows\n  - Processed 64000 rows\n  - Processed 64100 rows\n  - Processed 64200 rows\n  - Processed 64300 rows\n  - Processed 64400 rows\n  - Processed 64500 rows\n  - Processed 64600 rows\n  - Processed 64700 rows\n  - Processed 64800 rows\n  - Processed 64900 rows\n  - Processed 65000 rows\n  - Processed 65100 rows\n  - Processed 65200 rows\n  - Processed 65300 rows\n  - Processed 65400 rows\n  - Processed 65500 rows\n  - Processed 65600 rows\n  - Processed 65700 rows\n  - Processed 65800 rows\n  - Processed 65900 rows\n  - Processed 66000 rows\n  - Processed 66100 rows\n  - Processed 66200 rows\n  - Processed 66300 rows\n  - Processed 66400 rows\n  - Processed 66500 rows\n  - Processed 66600 rows\n  - Processed 66700 rows\n  - Processed 66800 rows\n  - Processed 66900 rows\n  - Processed 67000 rows\n  - Processed 67100 rows\n  - Processed 67200 rows\n  - Processed 67300 rows\n  - Processed 67400 rows\n  - Processed 67500 rows\n  - Processed 67600 rows\n  - Processed 67700 rows\n  - Processed 67800 rows\n  - Processed 67900 rows\n  - Processed 68000 rows\n  - Processed 68100 rows\n  - Processed 68200 rows\n  - Processed 68300 rows\n  - Processed 68400 rows\n  - Processed 68500 rows\n  - Processed 68600 rows\n  - Processed 68700 rows\n  - Processed 68800 rows\n  - Processed 68900 rows\n  - Processed 69000 rows\n  - Processed 69100 rows\n  - Processed 69200 rows\n  - Processed 69300 rows\n  - Processed 69400 rows\n  - Processed 69500 rows\n  - Processed 69600 rows\n  - Processed 69700 rows\n  - Processed 69800 rows\n  - Processed 69900 rows\n  - Processed 70000 rows\n  - Processed 70100 rows\n  - Processed 70200 rows\n  - Processed 70300 rows\n  - Processed 70400 rows\n  - Processed 70500 rows\n  - Processed 70600 rows\n  - Processed 70700 rows\n  - Processed 70800 rows\n  - Processed 70900 rows\n  - Processed 71000 rows\n  - Processed 71100 rows\n  - Processed 71200 rows\n  - Processed 71300 rows\n  - Processed 71400 rows\n  - Processed 71500 rows\n  - Processed 71600 rows\n  - Processed 71700 rows\n  - Processed 71800 rows\n  - Processed 71900 rows\n  - Processed 72000 rows\n  - Processed 72100 rows\n  - Processed 72200 rows\n  - Processed 72300 rows\n  - Processed 72400 rows\n  - Processed 72500 rows\n  - Processed 72600 rows\n  - Processed 72700 rows\n  - Processed 72800 rows\n  - Processed 72900 rows\n  - Processed 73000 rows\n  - Processed 73100 rows\n  - Processed 73200 rows\n  - Processed 73300 rows\n  - Processed 73400 rows\n  - Processed 73500 rows\n  - Processed 73600 rows\n  - Processed 73700 rows\n  - Processed 73800 rows\n  - Processed 73900 rows\n  - Processed 74000 rows\n  - Processed 74100 rows\n  - Processed 74200 rows\n  - Processed 74300 rows\n  - Processed 74400 rows\n  - Processed 74500 rows\n  - Processed 74600 rows\n  - Processed 74700 rows\n  - Processed 74800 rows\n  - Processed 74900 rows\n  - Processed 75000 rows\n  - Processed 75100 rows\n  - Processed 75200 rows\n  - Processed 75300 rows\n  - Processed 75400 rows\n  - Processed 75500 rows\n  - Processed 75600 rows\n  - Processed 75700 rows\n  - Processed 75800 rows\n  - Processed 75900 rows\n  - Processed 76000 rows\n  - Processed 76100 rows\n  - Processed 76200 rows\n  - Processed 76300 rows\n  - Processed 76400 rows\n  - Processed 76500 rows\n  - Processed 76600 rows\n  - Processed 76700 rows\n  - Processed 76800 rows\n  - Processed 76900 rows\n  - Processed 77000 rows\n  - Processed 77100 rows\n  - Processed 77200 rows\n  - Processed 77300 rows\n  - Processed 77400 rows\n  - Processed 77500 rows\n  - Processed 77600 rows\n  - Processed 77700 rows\n  - Processed 77800 rows\n  - Processed 77900 rows\n  - Processed 78000 rows\n  - Processed 78100 rows\n  - Processed 78200 rows\n  - Processed 78300 rows\n  - Processed 78400 rows\n  - Processed 78500 rows\n  - Processed 78600 rows\n  - Processed 78700 rows\n  - Processed 78800 rows\n  - Processed 78900 rows\n  - Processed 79000 rows\n  - Processed 79100 rows\n  - Processed 79200 rows\n  - Processed 79300 rows\n  - Processed 79400 rows\n  - Processed 79500 rows\n  - Processed 79600 rows\n  - Processed 79700 rows\n  - Processed 79800 rows\n  - Processed 79900 rows\n  - Processed 80000 rows\n  - Processed 80100 rows\n  - Processed 80200 rows\n  - Processed 80300 rows\n  - Processed 80400 rows\n  - Processed 80500 rows\n  - Processed 80600 rows\n\nExtracted 80600 ingredient lists and 80600 step lists\n\nTokenizing ingredients and steps...\n\nFlattening token structures...\n\nFlattening nested token structure...\n  - Flattened 100 recipes\n  - Flattened 200 recipes\n  - Flattened 300 recipes\n  - Flattened 400 recipes\n  - Flattened 500 recipes\n  - Flattened 600 recipes\n  - Flattened 700 recipes\n  - Flattened 800 recipes\n  - Flattened 900 recipes\n  - Flattened 1000 recipes\n  - Flattened 1100 recipes\n  - Flattened 1200 recipes\n  - Flattened 1300 recipes\n  - Flattened 1400 recipes\n  - Flattened 1500 recipes\n  - Flattened 1600 recipes\n  - Flattened 1700 recipes\n  - Flattened 1800 recipes\n  - Flattened 1900 recipes\n  - Flattened 2000 recipes\n  - Flattened 2100 recipes\n  - Flattened 2200 recipes\n  - Flattened 2300 recipes\n  - Flattened 2400 recipes\n  - Flattened 2500 recipes\n  - Flattened 2600 recipes\n  - Flattened 2700 recipes\n  - Flattened 2800 recipes\n  - Flattened 2900 recipes\n  - Flattened 3000 recipes\n  - Flattened 3100 recipes\n  - Flattened 3200 recipes\n  - Flattened 3300 recipes\n  - Flattened 3400 recipes\n  - Flattened 3500 recipes\n  - Flattened 3600 recipes\n  - Flattened 3700 recipes\n  - Flattened 3800 recipes\n  - Flattened 3900 recipes\n  - Flattened 4000 recipes\n  - Flattened 4100 recipes\n  - Flattened 4200 recipes\n  - Flattened 4300 recipes\n  - Flattened 4400 recipes\n  - Flattened 4500 recipes\n  - Flattened 4600 recipes\n  - Flattened 4700 recipes\n  - Flattened 4800 recipes\n  - Flattened 4900 recipes\n  - Flattened 5000 recipes\n  - Flattened 5100 recipes\n  - Flattened 5200 recipes\n  - Flattened 5300 recipes\n  - Flattened 5400 recipes\n  - Flattened 5500 recipes\n  - Flattened 5600 recipes\n  - Flattened 5700 recipes\n  - Flattened 5800 recipes\n  - Flattened 5900 recipes\n  - Flattened 6000 recipes\n  - Flattened 6100 recipes\n  - Flattened 6200 recipes\n  - Flattened 6300 recipes\n  - Flattened 6400 recipes\n  - Flattened 6500 recipes\n  - Flattened 6600 recipes\n  - Flattened 6700 recipes\n  - Flattened 6800 recipes\n  - Flattened 6900 recipes\n  - Flattened 7000 recipes\n  - Flattened 7100 recipes\n  - Flattened 7200 recipes\n  - Flattened 7300 recipes\n  - Flattened 7400 recipes\n  - Flattened 7500 recipes\n  - Flattened 7600 recipes\n  - Flattened 7700 recipes\n  - Flattened 7800 recipes\n  - Flattened 7900 recipes\n  - Flattened 8000 recipes\n  - Flattened 8100 recipes\n  - Flattened 8200 recipes\n  - Flattened 8300 recipes\n  - Flattened 8400 recipes\n  - Flattened 8500 recipes\n  - Flattened 8600 recipes\n  - Flattened 8700 recipes\n  - Flattened 8800 recipes\n  - Flattened 8900 recipes\n  - Flattened 9000 recipes\n  - Flattened 9100 recipes\n  - Flattened 9200 recipes\n  - Flattened 9300 recipes\n  - Flattened 9400 recipes\n  - Flattened 9500 recipes\n  - Flattened 9600 recipes\n  - Flattened 9700 recipes\n  - Flattened 9800 recipes\n  - Flattened 9900 recipes\n  - Flattened 10000 recipes\n  - Flattened 10100 recipes\n  - Flattened 10200 recipes\n  - Flattened 10300 recipes\n  - Flattened 10400 recipes\n  - Flattened 10500 recipes\n  - Flattened 10600 recipes\n  - Flattened 10700 recipes\n  - Flattened 10800 recipes\n  - Flattened 10900 recipes\n  - Flattened 11000 recipes\n  - Flattened 11100 recipes\n  - Flattened 11200 recipes\n  - Flattened 11300 recipes\n  - Flattened 11400 recipes\n  - Flattened 11500 recipes\n  - Flattened 11600 recipes\n  - Flattened 11700 recipes\n  - Flattened 11800 recipes\n  - Flattened 11900 recipes\n  - Flattened 12000 recipes\n  - Flattened 12100 recipes\n  - Flattened 12200 recipes\n  - Flattened 12300 recipes\n  - Flattened 12400 recipes\n  - Flattened 12500 recipes\n  - Flattened 12600 recipes\n  - Flattened 12700 recipes\n  - Flattened 12800 recipes\n  - Flattened 12900 recipes\n  - Flattened 13000 recipes\n  - Flattened 13100 recipes\n  - Flattened 13200 recipes\n  - Flattened 13300 recipes\n  - Flattened 13400 recipes\n  - Flattened 13500 recipes\n  - Flattened 13600 recipes\n  - Flattened 13700 recipes\n  - Flattened 13800 recipes\n  - Flattened 13900 recipes\n  - Flattened 14000 recipes\n  - Flattened 14100 recipes\n  - Flattened 14200 recipes\n  - Flattened 14300 recipes\n  - Flattened 14400 recipes\n  - Flattened 14500 recipes\n  - Flattened 14600 recipes\n  - Flattened 14700 recipes\n  - Flattened 14800 recipes\n  - Flattened 14900 recipes\n  - Flattened 15000 recipes\n  - Flattened 15100 recipes\n  - Flattened 15200 recipes\n  - Flattened 15300 recipes\n  - Flattened 15400 recipes\n  - Flattened 15500 recipes\n  - Flattened 15600 recipes\n  - Flattened 15700 recipes\n  - Flattened 15800 recipes\n  - Flattened 15900 recipes\n  - Flattened 16000 recipes\n  - Flattened 16100 recipes\n  - Flattened 16200 recipes\n  - Flattened 16300 recipes\n  - Flattened 16400 recipes\n  - Flattened 16500 recipes\n  - Flattened 16600 recipes\n  - Flattened 16700 recipes\n  - Flattened 16800 recipes\n  - Flattened 16900 recipes\n  - Flattened 17000 recipes\n  - Flattened 17100 recipes\n  - Flattened 17200 recipes\n  - Flattened 17300 recipes\n  - Flattened 17400 recipes\n  - Flattened 17500 recipes\n  - Flattened 17600 recipes\n  - Flattened 17700 recipes\n  - Flattened 17800 recipes\n  - Flattened 17900 recipes\n  - Flattened 18000 recipes\n  - Flattened 18100 recipes\n  - Flattened 18200 recipes\n  - Flattened 18300 recipes\n  - Flattened 18400 recipes\n  - Flattened 18500 recipes\n  - Flattened 18600 recipes\n  - Flattened 18700 recipes\n  - Flattened 18800 recipes\n  - Flattened 18900 recipes\n  - Flattened 19000 recipes\n  - Flattened 19100 recipes\n  - Flattened 19200 recipes\n  - Flattened 19300 recipes\n  - Flattened 19400 recipes\n  - Flattened 19500 recipes\n  - Flattened 19600 recipes\n  - Flattened 19700 recipes\n  - Flattened 19800 recipes\n  - Flattened 19900 recipes\n  - Flattened 20000 recipes\n  - Flattened 20100 recipes\n  - Flattened 20200 recipes\n  - Flattened 20300 recipes\n  - Flattened 20400 recipes\n  - Flattened 20500 recipes\n  - Flattened 20600 recipes\n  - Flattened 20700 recipes\n  - Flattened 20800 recipes\n  - Flattened 20900 recipes\n  - Flattened 21000 recipes\n  - Flattened 21100 recipes\n  - Flattened 21200 recipes\n  - Flattened 21300 recipes\n  - Flattened 21400 recipes\n  - Flattened 21500 recipes\n  - Flattened 21600 recipes\n  - Flattened 21700 recipes\n  - Flattened 21800 recipes\n  - Flattened 21900 recipes\n  - Flattened 22000 recipes\n  - Flattened 22100 recipes\n  - Flattened 22200 recipes\n  - Flattened 22300 recipes\n  - Flattened 22400 recipes\n  - Flattened 22500 recipes\n  - Flattened 22600 recipes\n  - Flattened 22700 recipes\n  - Flattened 22800 recipes\n  - Flattened 22900 recipes\n  - Flattened 23000 recipes\n  - Flattened 23100 recipes\n  - Flattened 23200 recipes\n  - Flattened 23300 recipes\n  - Flattened 23400 recipes\n  - Flattened 23500 recipes\n  - Flattened 23600 recipes\n  - Flattened 23700 recipes\n  - Flattened 23800 recipes\n  - Flattened 23900 recipes\n  - Flattened 24000 recipes\n  - Flattened 24100 recipes\n  - Flattened 24200 recipes\n  - Flattened 24300 recipes\n  - Flattened 24400 recipes\n  - Flattened 24500 recipes\n  - Flattened 24600 recipes\n  - Flattened 24700 recipes\n  - Flattened 24800 recipes\n  - Flattened 24900 recipes\n  - Flattened 25000 recipes\n  - Flattened 25100 recipes\n  - Flattened 25200 recipes\n  - Flattened 25300 recipes\n  - Flattened 25400 recipes\n  - Flattened 25500 recipes\n  - Flattened 25600 recipes\n  - Flattened 25700 recipes\n  - Flattened 25800 recipes\n  - Flattened 25900 recipes\n  - Flattened 26000 recipes\n  - Flattened 26100 recipes\n  - Flattened 26200 recipes\n  - Flattened 26300 recipes\n  - Flattened 26400 recipes\n  - Flattened 26500 recipes\n  - Flattened 26600 recipes\n  - Flattened 26700 recipes\n  - Flattened 26800 recipes\n  - Flattened 26900 recipes\n  - Flattened 27000 recipes\n  - Flattened 27100 recipes\n  - Flattened 27200 recipes\n  - Flattened 27300 recipes\n  - Flattened 27400 recipes\n  - Flattened 27500 recipes\n  - Flattened 27600 recipes\n  - Flattened 27700 recipes\n  - Flattened 27800 recipes\n  - Flattened 27900 recipes\n  - Flattened 28000 recipes\n  - Flattened 28100 recipes\n  - Flattened 28200 recipes\n  - Flattened 28300 recipes\n  - Flattened 28400 recipes\n  - Flattened 28500 recipes\n  - Flattened 28600 recipes\n  - Flattened 28700 recipes\n  - Flattened 28800 recipes\n  - Flattened 28900 recipes\n  - Flattened 29000 recipes\n  - Flattened 29100 recipes\n  - Flattened 29200 recipes\n  - Flattened 29300 recipes\n  - Flattened 29400 recipes\n  - Flattened 29500 recipes\n  - Flattened 29600 recipes\n  - Flattened 29700 recipes\n  - Flattened 29800 recipes\n  - Flattened 29900 recipes\n  - Flattened 30000 recipes\n  - Flattened 30100 recipes\n  - Flattened 30200 recipes\n  - Flattened 30300 recipes\n  - Flattened 30400 recipes\n  - Flattened 30500 recipes\n  - Flattened 30600 recipes\n  - Flattened 30700 recipes\n  - Flattened 30800 recipes\n  - Flattened 30900 recipes\n  - Flattened 31000 recipes\n  - Flattened 31100 recipes\n  - Flattened 31200 recipes\n  - Flattened 31300 recipes\n  - Flattened 31400 recipes\n  - Flattened 31500 recipes\n  - Flattened 31600 recipes\n  - Flattened 31700 recipes\n  - Flattened 31800 recipes\n  - Flattened 31900 recipes\n  - Flattened 32000 recipes\n  - Flattened 32100 recipes\n  - Flattened 32200 recipes\n  - Flattened 32300 recipes\n  - Flattened 32400 recipes\n  - Flattened 32500 recipes\n  - Flattened 32600 recipes\n  - Flattened 32700 recipes\n  - Flattened 32800 recipes\n  - Flattened 32900 recipes\n  - Flattened 33000 recipes\n  - Flattened 33100 recipes\n  - Flattened 33200 recipes\n  - Flattened 33300 recipes\n  - Flattened 33400 recipes\n  - Flattened 33500 recipes\n  - Flattened 33600 recipes\n  - Flattened 33700 recipes\n  - Flattened 33800 recipes\n  - Flattened 33900 recipes\n  - Flattened 34000 recipes\n  - Flattened 34100 recipes\n  - Flattened 34200 recipes\n  - Flattened 34300 recipes\n  - Flattened 34400 recipes\n  - Flattened 34500 recipes\n  - Flattened 34600 recipes\n  - Flattened 34700 recipes\n  - Flattened 34800 recipes\n  - Flattened 34900 recipes\n  - Flattened 35000 recipes\n  - Flattened 35100 recipes\n  - Flattened 35200 recipes\n  - Flattened 35300 recipes\n  - Flattened 35400 recipes\n  - Flattened 35500 recipes\n  - Flattened 35600 recipes\n  - Flattened 35700 recipes\n  - Flattened 35800 recipes\n  - Flattened 35900 recipes\n  - Flattened 36000 recipes\n  - Flattened 36100 recipes\n  - Flattened 36200 recipes\n  - Flattened 36300 recipes\n  - Flattened 36400 recipes\n  - Flattened 36500 recipes\n  - Flattened 36600 recipes\n  - Flattened 36700 recipes\n  - Flattened 36800 recipes\n  - Flattened 36900 recipes\n  - Flattened 37000 recipes\n  - Flattened 37100 recipes\n  - Flattened 37200 recipes\n  - Flattened 37300 recipes\n  - Flattened 37400 recipes\n  - Flattened 37500 recipes\n  - Flattened 37600 recipes\n  - Flattened 37700 recipes\n  - Flattened 37800 recipes\n  - Flattened 37900 recipes\n  - Flattened 38000 recipes\n  - Flattened 38100 recipes\n  - Flattened 38200 recipes\n  - Flattened 38300 recipes\n  - Flattened 38400 recipes\n  - Flattened 38500 recipes\n  - Flattened 38600 recipes\n  - Flattened 38700 recipes\n  - Flattened 38800 recipes\n  - Flattened 38900 recipes\n  - Flattened 39000 recipes\n  - Flattened 39100 recipes\n  - Flattened 39200 recipes\n  - Flattened 39300 recipes\n  - Flattened 39400 recipes\n  - Flattened 39500 recipes\n  - Flattened 39600 recipes\n  - Flattened 39700 recipes\n  - Flattened 39800 recipes\n  - Flattened 39900 recipes\n  - Flattened 40000 recipes\n  - Flattened 40100 recipes\n  - Flattened 40200 recipes\n  - Flattened 40300 recipes\n  - Flattened 40400 recipes\n  - Flattened 40500 recipes\n  - Flattened 40600 recipes\n  - Flattened 40700 recipes\n  - Flattened 40800 recipes\n  - Flattened 40900 recipes\n  - Flattened 41000 recipes\n  - Flattened 41100 recipes\n  - Flattened 41200 recipes\n  - Flattened 41300 recipes\n  - Flattened 41400 recipes\n  - Flattened 41500 recipes\n  - Flattened 41600 recipes\n  - Flattened 41700 recipes\n  - Flattened 41800 recipes\n  - Flattened 41900 recipes\n  - Flattened 42000 recipes\n  - Flattened 42100 recipes\n  - Flattened 42200 recipes\n  - Flattened 42300 recipes\n  - Flattened 42400 recipes\n  - Flattened 42500 recipes\n  - Flattened 42600 recipes\n  - Flattened 42700 recipes\n  - Flattened 42800 recipes\n  - Flattened 42900 recipes\n  - Flattened 43000 recipes\n  - Flattened 43100 recipes\n  - Flattened 43200 recipes\n  - Flattened 43300 recipes\n  - Flattened 43400 recipes\n  - Flattened 43500 recipes\n  - Flattened 43600 recipes\n  - Flattened 43700 recipes\n  - Flattened 43800 recipes\n  - Flattened 43900 recipes\n  - Flattened 44000 recipes\n  - Flattened 44100 recipes\n  - Flattened 44200 recipes\n  - Flattened 44300 recipes\n  - Flattened 44400 recipes\n  - Flattened 44500 recipes\n  - Flattened 44600 recipes\n  - Flattened 44700 recipes\n  - Flattened 44800 recipes\n  - Flattened 44900 recipes\n  - Flattened 45000 recipes\n  - Flattened 45100 recipes\n  - Flattened 45200 recipes\n  - Flattened 45300 recipes\n  - Flattened 45400 recipes\n  - Flattened 45500 recipes\n  - Flattened 45600 recipes\n  - Flattened 45700 recipes\n  - Flattened 45800 recipes\n  - Flattened 45900 recipes\n  - Flattened 46000 recipes\n  - Flattened 46100 recipes\n  - Flattened 46200 recipes\n  - Flattened 46300 recipes\n  - Flattened 46400 recipes\n  - Flattened 46500 recipes\n  - Flattened 46600 recipes\n  - Flattened 46700 recipes\n  - Flattened 46800 recipes\n  - Flattened 46900 recipes\n  - Flattened 47000 recipes\n  - Flattened 47100 recipes\n  - Flattened 47200 recipes\n  - Flattened 47300 recipes\n  - Flattened 47400 recipes\n  - Flattened 47500 recipes\n  - Flattened 47600 recipes\n  - Flattened 47700 recipes\n  - Flattened 47800 recipes\n  - Flattened 47900 recipes\n  - Flattened 48000 recipes\n  - Flattened 48100 recipes\n  - Flattened 48200 recipes\n  - Flattened 48300 recipes\n  - Flattened 48400 recipes\n  - Flattened 48500 recipes\n  - Flattened 48600 recipes\n  - Flattened 48700 recipes\n  - Flattened 48800 recipes\n  - Flattened 48900 recipes\n  - Flattened 49000 recipes\n  - Flattened 49100 recipes\n  - Flattened 49200 recipes\n  - Flattened 49300 recipes\n  - Flattened 49400 recipes\n  - Flattened 49500 recipes\n  - Flattened 49600 recipes\n  - Flattened 49700 recipes\n  - Flattened 49800 recipes\n  - Flattened 49900 recipes\n  - Flattened 50000 recipes\n  - Flattened 50100 recipes\n  - Flattened 50200 recipes\n  - Flattened 50300 recipes\n  - Flattened 50400 recipes\n  - Flattened 50500 recipes\n  - Flattened 50600 recipes\n  - Flattened 50700 recipes\n  - Flattened 50800 recipes\n  - Flattened 50900 recipes\n  - Flattened 51000 recipes\n  - Flattened 51100 recipes\n  - Flattened 51200 recipes\n  - Flattened 51300 recipes\n  - Flattened 51400 recipes\n  - Flattened 51500 recipes\n  - Flattened 51600 recipes\n  - Flattened 51700 recipes\n  - Flattened 51800 recipes\n  - Flattened 51900 recipes\n  - Flattened 52000 recipes\n  - Flattened 52100 recipes\n  - Flattened 52200 recipes\n  - Flattened 52300 recipes\n  - Flattened 52400 recipes\n  - Flattened 52500 recipes\n  - Flattened 52600 recipes\n  - Flattened 52700 recipes\n  - Flattened 52800 recipes\n  - Flattened 52900 recipes\n  - Flattened 53000 recipes\n  - Flattened 53100 recipes\n  - Flattened 53200 recipes\n  - Flattened 53300 recipes\n  - Flattened 53400 recipes\n  - Flattened 53500 recipes\n  - Flattened 53600 recipes\n  - Flattened 53700 recipes\n  - Flattened 53800 recipes\n  - Flattened 53900 recipes\n  - Flattened 54000 recipes\n  - Flattened 54100 recipes\n  - Flattened 54200 recipes\n  - Flattened 54300 recipes\n  - Flattened 54400 recipes\n  - Flattened 54500 recipes\n  - Flattened 54600 recipes\n  - Flattened 54700 recipes\n  - Flattened 54800 recipes\n  - Flattened 54900 recipes\n  - Flattened 55000 recipes\n  - Flattened 55100 recipes\n  - Flattened 55200 recipes\n  - Flattened 55300 recipes\n  - Flattened 55400 recipes\n  - Flattened 55500 recipes\n  - Flattened 55600 recipes\n  - Flattened 55700 recipes\n  - Flattened 55800 recipes\n  - Flattened 55900 recipes\n  - Flattened 56000 recipes\n  - Flattened 56100 recipes\n  - Flattened 56200 recipes\n  - Flattened 56300 recipes\n  - Flattened 56400 recipes\n  - Flattened 56500 recipes\n  - Flattened 56600 recipes\n  - Flattened 56700 recipes\n  - Flattened 56800 recipes\n  - Flattened 56900 recipes\n  - Flattened 57000 recipes\n  - Flattened 57100 recipes\n  - Flattened 57200 recipes\n  - Flattened 57300 recipes\n  - Flattened 57400 recipes\n  - Flattened 57500 recipes\n  - Flattened 57600 recipes\n  - Flattened 57700 recipes\n  - Flattened 57800 recipes\n  - Flattened 57900 recipes\n  - Flattened 58000 recipes\n  - Flattened 58100 recipes\n  - Flattened 58200 recipes\n  - Flattened 58300 recipes\n  - Flattened 58400 recipes\n  - Flattened 58500 recipes\n  - Flattened 58600 recipes\n  - Flattened 58700 recipes\n  - Flattened 58800 recipes\n  - Flattened 58900 recipes\n  - Flattened 59000 recipes\n  - Flattened 59100 recipes\n  - Flattened 59200 recipes\n  - Flattened 59300 recipes\n  - Flattened 59400 recipes\n  - Flattened 59500 recipes\n  - Flattened 59600 recipes\n  - Flattened 59700 recipes\n  - Flattened 59800 recipes\n  - Flattened 59900 recipes\n  - Flattened 60000 recipes\n  - Flattened 60100 recipes\n  - Flattened 60200 recipes\n  - Flattened 60300 recipes\n  - Flattened 60400 recipes\n  - Flattened 60500 recipes\n  - Flattened 60600 recipes\n  - Flattened 60700 recipes\n  - Flattened 60800 recipes\n  - Flattened 60900 recipes\n  - Flattened 61000 recipes\n  - Flattened 61100 recipes\n  - Flattened 61200 recipes\n  - Flattened 61300 recipes\n  - Flattened 61400 recipes\n  - Flattened 61500 recipes\n  - Flattened 61600 recipes\n  - Flattened 61700 recipes\n  - Flattened 61800 recipes\n  - Flattened 61900 recipes\n  - Flattened 62000 recipes\n  - Flattened 62100 recipes\n  - Flattened 62200 recipes\n  - Flattened 62300 recipes\n  - Flattened 62400 recipes\n  - Flattened 62500 recipes\n  - Flattened 62600 recipes\n  - Flattened 62700 recipes\n  - Flattened 62800 recipes\n  - Flattened 62900 recipes\n  - Flattened 63000 recipes\n  - Flattened 63100 recipes\n  - Flattened 63200 recipes\n  - Flattened 63300 recipes\n  - Flattened 63400 recipes\n  - Flattened 63500 recipes\n  - Flattened 63600 recipes\n  - Flattened 63700 recipes\n  - Flattened 63800 recipes\n  - Flattened 63900 recipes\n  - Flattened 64000 recipes\n  - Flattened 64100 recipes\n  - Flattened 64200 recipes\n  - Flattened 64300 recipes\n  - Flattened 64400 recipes\n  - Flattened 64500 recipes\n  - Flattened 64600 recipes\n  - Flattened 64700 recipes\n  - Flattened 64800 recipes\n  - Flattened 64900 recipes\n  - Flattened 65000 recipes\n  - Flattened 65100 recipes\n  - Flattened 65200 recipes\n  - Flattened 65300 recipes\n  - Flattened 65400 recipes\n  - Flattened 65500 recipes\n  - Flattened 65600 recipes\n  - Flattened 65700 recipes\n  - Flattened 65800 recipes\n  - Flattened 65900 recipes\n  - Flattened 66000 recipes\n  - Flattened 66100 recipes\n  - Flattened 66200 recipes\n  - Flattened 66300 recipes\n  - Flattened 66400 recipes\n  - Flattened 66500 recipes\n  - Flattened 66600 recipes\n  - Flattened 66700 recipes\n  - Flattened 66800 recipes\n  - Flattened 66900 recipes\n  - Flattened 67000 recipes\n  - Flattened 67100 recipes\n  - Flattened 67200 recipes\n  - Flattened 67300 recipes\n  - Flattened 67400 recipes\n  - Flattened 67500 recipes\n  - Flattened 67600 recipes\n  - Flattened 67700 recipes\n  - Flattened 67800 recipes\n  - Flattened 67900 recipes\n  - Flattened 68000 recipes\n  - Flattened 68100 recipes\n  - Flattened 68200 recipes\n  - Flattened 68300 recipes\n  - Flattened 68400 recipes\n  - Flattened 68500 recipes\n  - Flattened 68600 recipes\n  - Flattened 68700 recipes\n  - Flattened 68800 recipes\n  - Flattened 68900 recipes\n  - Flattened 69000 recipes\n  - Flattened 69100 recipes\n  - Flattened 69200 recipes\n  - Flattened 69300 recipes\n  - Flattened 69400 recipes\n  - Flattened 69500 recipes\n  - Flattened 69600 recipes\n  - Flattened 69700 recipes\n  - Flattened 69800 recipes\n  - Flattened 69900 recipes\n  - Flattened 70000 recipes\n  - Flattened 70100 recipes\n  - Flattened 70200 recipes\n  - Flattened 70300 recipes\n  - Flattened 70400 recipes\n  - Flattened 70500 recipes\n  - Flattened 70600 recipes\n  - Flattened 70700 recipes\n  - Flattened 70800 recipes\n  - Flattened 70900 recipes\n  - Flattened 71000 recipes\n  - Flattened 71100 recipes\n  - Flattened 71200 recipes\n  - Flattened 71300 recipes\n  - Flattened 71400 recipes\n  - Flattened 71500 recipes\n  - Flattened 71600 recipes\n  - Flattened 71700 recipes\n  - Flattened 71800 recipes\n  - Flattened 71900 recipes\n  - Flattened 72000 recipes\n  - Flattened 72100 recipes\n  - Flattened 72200 recipes\n  - Flattened 72300 recipes\n  - Flattened 72400 recipes\n  - Flattened 72500 recipes\n  - Flattened 72600 recipes\n  - Flattened 72700 recipes\n  - Flattened 72800 recipes\n  - Flattened 72900 recipes\n  - Flattened 73000 recipes\n  - Flattened 73100 recipes\n  - Flattened 73200 recipes\n  - Flattened 73300 recipes\n  - Flattened 73400 recipes\n  - Flattened 73500 recipes\n  - Flattened 73600 recipes\n  - Flattened 73700 recipes\n  - Flattened 73800 recipes\n  - Flattened 73900 recipes\n  - Flattened 74000 recipes\n  - Flattened 74100 recipes\n  - Flattened 74200 recipes\n  - Flattened 74300 recipes\n  - Flattened 74400 recipes\n  - Flattened 74500 recipes\n  - Flattened 74600 recipes\n  - Flattened 74700 recipes\n  - Flattened 74800 recipes\n  - Flattened 74900 recipes\n  - Flattened 75000 recipes\n  - Flattened 75100 recipes\n  - Flattened 75200 recipes\n  - Flattened 75300 recipes\n  - Flattened 75400 recipes\n  - Flattened 75500 recipes\n  - Flattened 75600 recipes\n  - Flattened 75700 recipes\n  - Flattened 75800 recipes\n  - Flattened 75900 recipes\n  - Flattened 76000 recipes\n  - Flattened 76100 recipes\n  - Flattened 76200 recipes\n  - Flattened 76300 recipes\n  - Flattened 76400 recipes\n  - Flattened 76500 recipes\n  - Flattened 76600 recipes\n  - Flattened 76700 recipes\n  - Flattened 76800 recipes\n  - Flattened 76900 recipes\n  - Flattened 77000 recipes\n  - Flattened 77100 recipes\n  - Flattened 77200 recipes\n  - Flattened 77300 recipes\n  - Flattened 77400 recipes\n  - Flattened 77500 recipes\n  - Flattened 77600 recipes\n  - Flattened 77700 recipes\n  - Flattened 77800 recipes\n  - Flattened 77900 recipes\n  - Flattened 78000 recipes\n  - Flattened 78100 recipes\n  - Flattened 78200 recipes\n  - Flattened 78300 recipes\n  - Flattened 78400 recipes\n  - Flattened 78500 recipes\n  - Flattened 78600 recipes\n  - Flattened 78700 recipes\n  - Flattened 78800 recipes\n  - Flattened 78900 recipes\n  - Flattened 79000 recipes\n  - Flattened 79100 recipes\n  - Flattened 79200 recipes\n  - Flattened 79300 recipes\n  - Flattened 79400 recipes\n  - Flattened 79500 recipes\n  - Flattened 79600 recipes\n  - Flattened 79700 recipes\n  - Flattened 79800 recipes\n  - Flattened 79900 recipes\n  - Flattened 80000 recipes\n  - Flattened 80100 recipes\n  - Flattened 80200 recipes\n  - Flattened 80300 recipes\n  - Flattened 80400 recipes\n  - Flattened 80500 recipes\nFlattening complete. Converted 80600 nested lists to flat token lists.\n\nFlattening nested token structure...\n  - Flattened 100 recipes\n  - Flattened 200 recipes\n  - Flattened 300 recipes\n  - Flattened 400 recipes\n  - Flattened 500 recipes\n  - Flattened 600 recipes\n  - Flattened 700 recipes\n  - Flattened 800 recipes\n  - Flattened 900 recipes\n  - Flattened 1000 recipes\n  - Flattened 1100 recipes\n  - Flattened 1200 recipes\n  - Flattened 1300 recipes\n  - Flattened 1400 recipes\n  - Flattened 1500 recipes\n  - Flattened 1600 recipes\n  - Flattened 1700 recipes\n  - Flattened 1800 recipes\n  - Flattened 1900 recipes\n  - Flattened 2000 recipes\n  - Flattened 2100 recipes\n  - Flattened 2200 recipes\n  - Flattened 2300 recipes\n  - Flattened 2400 recipes\n  - Flattened 2500 recipes\n  - Flattened 2600 recipes\n  - Flattened 2700 recipes\n  - Flattened 2800 recipes\n  - Flattened 2900 recipes\n  - Flattened 3000 recipes\n  - Flattened 3100 recipes\n  - Flattened 3200 recipes\n  - Flattened 3300 recipes\n  - Flattened 3400 recipes\n  - Flattened 3500 recipes\n  - Flattened 3600 recipes\n  - Flattened 3700 recipes\n  - Flattened 3800 recipes\n  - Flattened 3900 recipes\n  - Flattened 4000 recipes\n  - Flattened 4100 recipes\n  - Flattened 4200 recipes\n  - Flattened 4300 recipes\n  - Flattened 4400 recipes\n  - Flattened 4500 recipes\n  - Flattened 4600 recipes\n  - Flattened 4700 recipes\n  - Flattened 4800 recipes\n  - Flattened 4900 recipes\n  - Flattened 5000 recipes\n  - Flattened 5100 recipes\n  - Flattened 5200 recipes\n  - Flattened 5300 recipes\n  - Flattened 5400 recipes\n  - Flattened 5500 recipes\n  - Flattened 5600 recipes\n  - Flattened 5700 recipes\n  - Flattened 5800 recipes\n  - Flattened 5900 recipes\n  - Flattened 6000 recipes\n  - Flattened 6100 recipes\n  - Flattened 6200 recipes\n  - Flattened 6300 recipes\n  - Flattened 6400 recipes\n  - Flattened 6500 recipes\n  - Flattened 6600 recipes\n  - Flattened 6700 recipes\n  - Flattened 6800 recipes\n  - Flattened 6900 recipes\n  - Flattened 7000 recipes\n  - Flattened 7100 recipes\n  - Flattened 7200 recipes\n  - Flattened 7300 recipes\n  - Flattened 7400 recipes\n  - Flattened 7500 recipes\n  - Flattened 7600 recipes\n  - Flattened 7700 recipes\n  - Flattened 7800 recipes\n  - Flattened 7900 recipes\n  - Flattened 8000 recipes\n  - Flattened 8100 recipes\n  - Flattened 8200 recipes\n  - Flattened 8300 recipes\n  - Flattened 8400 recipes\n  - Flattened 8500 recipes\n  - Flattened 8600 recipes\n  - Flattened 8700 recipes\n  - Flattened 8800 recipes\n  - Flattened 8900 recipes\n  - Flattened 9000 recipes\n  - Flattened 9100 recipes\n  - Flattened 9200 recipes\n  - Flattened 9300 recipes\n  - Flattened 9400 recipes\n  - Flattened 9500 recipes\n  - Flattened 9600 recipes\n  - Flattened 9700 recipes\n  - Flattened 9800 recipes\n  - Flattened 9900 recipes\n  - Flattened 10000 recipes\n  - Flattened 10100 recipes\n  - Flattened 10200 recipes\n  - Flattened 10300 recipes\n  - Flattened 10400 recipes\n  - Flattened 10500 recipes\n  - Flattened 10600 recipes\n  - Flattened 10700 recipes\n  - Flattened 10800 recipes\n  - Flattened 10900 recipes\n  - Flattened 11000 recipes\n  - Flattened 11100 recipes\n  - Flattened 11200 recipes\n  - Flattened 11300 recipes\n  - Flattened 11400 recipes\n  - Flattened 11500 recipes\n  - Flattened 11600 recipes\n  - Flattened 11700 recipes\n  - Flattened 11800 recipes\n  - Flattened 11900 recipes\n  - Flattened 12000 recipes\n  - Flattened 12100 recipes\n  - Flattened 12200 recipes\n  - Flattened 12300 recipes\n  - Flattened 12400 recipes\n  - Flattened 12500 recipes\n  - Flattened 12600 recipes\n  - Flattened 12700 recipes\n  - Flattened 12800 recipes\n  - Flattened 12900 recipes\n  - Flattened 13000 recipes\n  - Flattened 13100 recipes\n  - Flattened 13200 recipes\n  - Flattened 13300 recipes\n  - Flattened 13400 recipes\n  - Flattened 13500 recipes\n  - Flattened 13600 recipes\n  - Flattened 13700 recipes\n  - Flattened 13800 recipes\n  - Flattened 13900 recipes\n  - Flattened 14000 recipes\n  - Flattened 14100 recipes\n  - Flattened 14200 recipes\n  - Flattened 14300 recipes\n  - Flattened 14400 recipes\n  - Flattened 14500 recipes\n  - Flattened 14600 recipes\n  - Flattened 14700 recipes\n  - Flattened 14800 recipes\n  - Flattened 14900 recipes\n  - Flattened 15000 recipes\n  - Flattened 15100 recipes\n  - Flattened 15200 recipes\n  - Flattened 15300 recipes\n  - Flattened 15400 recipes\n  - Flattened 15500 recipes\n  - Flattened 15600 recipes\n  - Flattened 15700 recipes\n  - Flattened 15800 recipes\n  - Flattened 15900 recipes\n  - Flattened 16000 recipes\n  - Flattened 16100 recipes\n  - Flattened 16200 recipes\n  - Flattened 16300 recipes\n  - Flattened 16400 recipes\n  - Flattened 16500 recipes\n  - Flattened 16600 recipes\n  - Flattened 16700 recipes\n  - Flattened 16800 recipes\n  - Flattened 16900 recipes\n  - Flattened 17000 recipes\n  - Flattened 17100 recipes\n  - Flattened 17200 recipes\n  - Flattened 17300 recipes\n  - Flattened 17400 recipes\n  - Flattened 17500 recipes\n  - Flattened 17600 recipes\n  - Flattened 17700 recipes\n  - Flattened 17800 recipes\n  - Flattened 17900 recipes\n  - Flattened 18000 recipes\n  - Flattened 18100 recipes\n  - Flattened 18200 recipes\n  - Flattened 18300 recipes\n  - Flattened 18400 recipes\n  - Flattened 18500 recipes\n  - Flattened 18600 recipes\n  - Flattened 18700 recipes\n  - Flattened 18800 recipes\n  - Flattened 18900 recipes\n  - Flattened 19000 recipes\n  - Flattened 19100 recipes\n  - Flattened 19200 recipes\n  - Flattened 19300 recipes\n  - Flattened 19400 recipes\n  - Flattened 19500 recipes\n  - Flattened 19600 recipes\n  - Flattened 19700 recipes\n  - Flattened 19800 recipes\n  - Flattened 19900 recipes\n  - Flattened 20000 recipes\n  - Flattened 20100 recipes\n  - Flattened 20200 recipes\n  - Flattened 20300 recipes\n  - Flattened 20400 recipes\n  - Flattened 20500 recipes\n  - Flattened 20600 recipes\n  - Flattened 20700 recipes\n  - Flattened 20800 recipes\n  - Flattened 20900 recipes\n  - Flattened 21000 recipes\n  - Flattened 21100 recipes\n  - Flattened 21200 recipes\n  - Flattened 21300 recipes\n  - Flattened 21400 recipes\n  - Flattened 21500 recipes\n  - Flattened 21600 recipes\n  - Flattened 21700 recipes\n  - Flattened 21800 recipes\n  - Flattened 21900 recipes\n  - Flattened 22000 recipes\n  - Flattened 22100 recipes\n  - Flattened 22200 recipes\n  - Flattened 22300 recipes\n  - Flattened 22400 recipes\n  - Flattened 22500 recipes\n  - Flattened 22600 recipes\n  - Flattened 22700 recipes\n  - Flattened 22800 recipes\n  - Flattened 22900 recipes\n  - Flattened 23000 recipes\n  - Flattened 23100 recipes\n  - Flattened 23200 recipes\n  - Flattened 23300 recipes\n  - Flattened 23400 recipes\n  - Flattened 23500 recipes\n  - Flattened 23600 recipes\n  - Flattened 23700 recipes\n  - Flattened 23800 recipes\n  - Flattened 23900 recipes\n  - Flattened 24000 recipes\n  - Flattened 24100 recipes\n  - Flattened 24200 recipes\n  - Flattened 24300 recipes\n  - Flattened 24400 recipes\n  - Flattened 24500 recipes\n  - Flattened 24600 recipes\n  - Flattened 24700 recipes\n  - Flattened 24800 recipes\n  - Flattened 24900 recipes\n  - Flattened 25000 recipes\n  - Flattened 25100 recipes\n  - Flattened 25200 recipes\n  - Flattened 25300 recipes\n  - Flattened 25400 recipes\n  - Flattened 25500 recipes\n  - Flattened 25600 recipes\n  - Flattened 25700 recipes\n  - Flattened 25800 recipes\n  - Flattened 25900 recipes\n  - Flattened 26000 recipes\n  - Flattened 26100 recipes\n  - Flattened 26200 recipes\n  - Flattened 26300 recipes\n  - Flattened 26400 recipes\n  - Flattened 26500 recipes\n  - Flattened 26600 recipes\n  - Flattened 26700 recipes\n  - Flattened 26800 recipes\n  - Flattened 26900 recipes\n  - Flattened 27000 recipes\n  - Flattened 27100 recipes\n  - Flattened 27200 recipes\n  - Flattened 27300 recipes\n  - Flattened 27400 recipes\n  - Flattened 27500 recipes\n  - Flattened 27600 recipes\n  - Flattened 27700 recipes\n  - Flattened 27800 recipes\n  - Flattened 27900 recipes\n  - Flattened 28000 recipes\n  - Flattened 28100 recipes\n  - Flattened 28200 recipes\n  - Flattened 28300 recipes\n  - Flattened 28400 recipes\n  - Flattened 28500 recipes\n  - Flattened 28600 recipes\n  - Flattened 28700 recipes\n  - Flattened 28800 recipes\n  - Flattened 28900 recipes\n  - Flattened 29000 recipes\n  - Flattened 29100 recipes\n  - Flattened 29200 recipes\n  - Flattened 29300 recipes\n  - Flattened 29400 recipes\n  - Flattened 29500 recipes\n  - Flattened 29600 recipes\n  - Flattened 29700 recipes\n  - Flattened 29800 recipes\n  - Flattened 29900 recipes\n  - Flattened 30000 recipes\n  - Flattened 30100 recipes\n  - Flattened 30200 recipes\n  - Flattened 30300 recipes\n  - Flattened 30400 recipes\n  - Flattened 30500 recipes\n  - Flattened 30600 recipes\n  - Flattened 30700 recipes\n  - Flattened 30800 recipes\n  - Flattened 30900 recipes\n  - Flattened 31000 recipes\n  - Flattened 31100 recipes\n  - Flattened 31200 recipes\n  - Flattened 31300 recipes\n  - Flattened 31400 recipes\n  - Flattened 31500 recipes\n  - Flattened 31600 recipes\n  - Flattened 31700 recipes\n  - Flattened 31800 recipes\n  - Flattened 31900 recipes\n  - Flattened 32000 recipes\n  - Flattened 32100 recipes\n  - Flattened 32200 recipes\n  - Flattened 32300 recipes\n  - Flattened 32400 recipes\n  - Flattened 32500 recipes\n  - Flattened 32600 recipes\n  - Flattened 32700 recipes\n  - Flattened 32800 recipes\n  - Flattened 32900 recipes\n  - Flattened 33000 recipes\n  - Flattened 33100 recipes\n  - Flattened 33200 recipes\n  - Flattened 33300 recipes\n  - Flattened 33400 recipes\n  - Flattened 33500 recipes\n  - Flattened 33600 recipes\n  - Flattened 33700 recipes\n  - Flattened 33800 recipes\n  - Flattened 33900 recipes\n  - Flattened 34000 recipes\n  - Flattened 34100 recipes\n  - Flattened 34200 recipes\n  - Flattened 34300 recipes\n  - Flattened 34400 recipes\n  - Flattened 34500 recipes\n  - Flattened 34600 recipes\n  - Flattened 34700 recipes\n  - Flattened 34800 recipes\n  - Flattened 34900 recipes\n  - Flattened 35000 recipes\n  - Flattened 35100 recipes\n  - Flattened 35200 recipes\n  - Flattened 35300 recipes\n  - Flattened 35400 recipes\n  - Flattened 35500 recipes\n  - Flattened 35600 recipes\n  - Flattened 35700 recipes\n  - Flattened 35800 recipes\n  - Flattened 35900 recipes\n  - Flattened 36000 recipes\n  - Flattened 36100 recipes\n  - Flattened 36200 recipes\n  - Flattened 36300 recipes\n  - Flattened 36400 recipes\n  - Flattened 36500 recipes\n  - Flattened 36600 recipes\n  - Flattened 36700 recipes\n  - Flattened 36800 recipes\n  - Flattened 36900 recipes\n  - Flattened 37000 recipes\n  - Flattened 37100 recipes\n  - Flattened 37200 recipes\n  - Flattened 37300 recipes\n  - Flattened 37400 recipes\n  - Flattened 37500 recipes\n  - Flattened 37600 recipes\n  - Flattened 37700 recipes\n  - Flattened 37800 recipes\n  - Flattened 37900 recipes\n  - Flattened 38000 recipes\n  - Flattened 38100 recipes\n  - Flattened 38200 recipes\n  - Flattened 38300 recipes\n  - Flattened 38400 recipes\n  - Flattened 38500 recipes\n  - Flattened 38600 recipes\n  - Flattened 38700 recipes\n  - Flattened 38800 recipes\n  - Flattened 38900 recipes\n  - Flattened 39000 recipes\n  - Flattened 39100 recipes\n  - Flattened 39200 recipes\n  - Flattened 39300 recipes\n  - Flattened 39400 recipes\n  - Flattened 39500 recipes\n  - Flattened 39600 recipes\n  - Flattened 39700 recipes\n  - Flattened 39800 recipes\n  - Flattened 39900 recipes\n  - Flattened 40000 recipes\n  - Flattened 40100 recipes\n  - Flattened 40200 recipes\n  - Flattened 40300 recipes\n  - Flattened 40400 recipes\n  - Flattened 40500 recipes\n  - Flattened 40600 recipes\n  - Flattened 40700 recipes\n  - Flattened 40800 recipes\n  - Flattened 40900 recipes\n  - Flattened 41000 recipes\n  - Flattened 41100 recipes\n  - Flattened 41200 recipes\n  - Flattened 41300 recipes\n  - Flattened 41400 recipes\n  - Flattened 41500 recipes\n  - Flattened 41600 recipes\n  - Flattened 41700 recipes\n  - Flattened 41800 recipes\n  - Flattened 41900 recipes\n  - Flattened 42000 recipes\n  - Flattened 42100 recipes\n  - Flattened 42200 recipes\n  - Flattened 42300 recipes\n  - Flattened 42400 recipes\n  - Flattened 42500 recipes\n  - Flattened 42600 recipes\n  - Flattened 42700 recipes\n  - Flattened 42800 recipes\n  - Flattened 42900 recipes\n  - Flattened 43000 recipes\n  - Flattened 43100 recipes\n  - Flattened 43200 recipes\n  - Flattened 43300 recipes\n  - Flattened 43400 recipes\n  - Flattened 43500 recipes\n  - Flattened 43600 recipes\n  - Flattened 43700 recipes\n  - Flattened 43800 recipes\n  - Flattened 43900 recipes\n  - Flattened 44000 recipes\n  - Flattened 44100 recipes\n  - Flattened 44200 recipes\n  - Flattened 44300 recipes\n  - Flattened 44400 recipes\n  - Flattened 44500 recipes\n  - Flattened 44600 recipes\n  - Flattened 44700 recipes\n  - Flattened 44800 recipes\n  - Flattened 44900 recipes\n  - Flattened 45000 recipes\n  - Flattened 45100 recipes\n  - Flattened 45200 recipes\n  - Flattened 45300 recipes\n  - Flattened 45400 recipes\n  - Flattened 45500 recipes\n  - Flattened 45600 recipes\n  - Flattened 45700 recipes\n  - Flattened 45800 recipes\n  - Flattened 45900 recipes\n  - Flattened 46000 recipes\n  - Flattened 46100 recipes\n  - Flattened 46200 recipes\n  - Flattened 46300 recipes\n  - Flattened 46400 recipes\n  - Flattened 46500 recipes\n  - Flattened 46600 recipes\n  - Flattened 46700 recipes\n  - Flattened 46800 recipes\n  - Flattened 46900 recipes\n  - Flattened 47000 recipes\n  - Flattened 47100 recipes\n  - Flattened 47200 recipes\n  - Flattened 47300 recipes\n  - Flattened 47400 recipes\n  - Flattened 47500 recipes\n  - Flattened 47600 recipes\n  - Flattened 47700 recipes\n  - Flattened 47800 recipes\n  - Flattened 47900 recipes\n  - Flattened 48000 recipes\n  - Flattened 48100 recipes\n  - Flattened 48200 recipes\n  - Flattened 48300 recipes\n  - Flattened 48400 recipes\n  - Flattened 48500 recipes\n  - Flattened 48600 recipes\n  - Flattened 48700 recipes\n  - Flattened 48800 recipes\n  - Flattened 48900 recipes\n  - Flattened 49000 recipes\n  - Flattened 49100 recipes\n  - Flattened 49200 recipes\n  - Flattened 49300 recipes\n  - Flattened 49400 recipes\n  - Flattened 49500 recipes\n  - Flattened 49600 recipes\n  - Flattened 49700 recipes\n  - Flattened 49800 recipes\n  - Flattened 49900 recipes\n  - Flattened 50000 recipes\n  - Flattened 50100 recipes\n  - Flattened 50200 recipes\n  - Flattened 50300 recipes\n  - Flattened 50400 recipes\n  - Flattened 50500 recipes\n  - Flattened 50600 recipes\n  - Flattened 50700 recipes\n  - Flattened 50800 recipes\n  - Flattened 50900 recipes\n  - Flattened 51000 recipes\n  - Flattened 51100 recipes\n  - Flattened 51200 recipes\n  - Flattened 51300 recipes\n  - Flattened 51400 recipes\n  - Flattened 51500 recipes\n  - Flattened 51600 recipes\n  - Flattened 51700 recipes\n  - Flattened 51800 recipes\n  - Flattened 51900 recipes\n  - Flattened 52000 recipes\n  - Flattened 52100 recipes\n  - Flattened 52200 recipes\n  - Flattened 52300 recipes\n  - Flattened 52400 recipes\n  - Flattened 52500 recipes\n  - Flattened 52600 recipes\n  - Flattened 52700 recipes\n  - Flattened 52800 recipes\n  - Flattened 52900 recipes\n  - Flattened 53000 recipes\n  - Flattened 53100 recipes\n  - Flattened 53200 recipes\n  - Flattened 53300 recipes\n  - Flattened 53400 recipes\n  - Flattened 53500 recipes\n  - Flattened 53600 recipes\n  - Flattened 53700 recipes\n  - Flattened 53800 recipes\n  - Flattened 53900 recipes\n  - Flattened 54000 recipes\n  - Flattened 54100 recipes\n  - Flattened 54200 recipes\n  - Flattened 54300 recipes\n  - Flattened 54400 recipes\n  - Flattened 54500 recipes\n  - Flattened 54600 recipes\n  - Flattened 54700 recipes\n  - Flattened 54800 recipes\n  - Flattened 54900 recipes\n  - Flattened 55000 recipes\n  - Flattened 55100 recipes\n  - Flattened 55200 recipes\n  - Flattened 55300 recipes\n  - Flattened 55400 recipes\n  - Flattened 55500 recipes\n  - Flattened 55600 recipes\n  - Flattened 55700 recipes\n  - Flattened 55800 recipes\n  - Flattened 55900 recipes\n  - Flattened 56000 recipes\n  - Flattened 56100 recipes\n  - Flattened 56200 recipes\n  - Flattened 56300 recipes\n  - Flattened 56400 recipes\n  - Flattened 56500 recipes\n  - Flattened 56600 recipes\n  - Flattened 56700 recipes\n  - Flattened 56800 recipes\n  - Flattened 56900 recipes\n  - Flattened 57000 recipes\n  - Flattened 57100 recipes\n  - Flattened 57200 recipes\n  - Flattened 57300 recipes\n  - Flattened 57400 recipes\n  - Flattened 57500 recipes\n  - Flattened 57600 recipes\n  - Flattened 57700 recipes\n  - Flattened 57800 recipes\n  - Flattened 57900 recipes\n  - Flattened 58000 recipes\n  - Flattened 58100 recipes\n  - Flattened 58200 recipes\n  - Flattened 58300 recipes\n  - Flattened 58400 recipes\n  - Flattened 58500 recipes\n  - Flattened 58600 recipes\n  - Flattened 58700 recipes\n  - Flattened 58800 recipes\n  - Flattened 58900 recipes\n  - Flattened 59000 recipes\n  - Flattened 59100 recipes\n  - Flattened 59200 recipes\n  - Flattened 59300 recipes\n  - Flattened 59400 recipes\n  - Flattened 59500 recipes\n  - Flattened 59600 recipes\n  - Flattened 59700 recipes\n  - Flattened 59800 recipes\n  - Flattened 59900 recipes\n  - Flattened 60000 recipes\n  - Flattened 60100 recipes\n  - Flattened 60200 recipes\n  - Flattened 60300 recipes\n  - Flattened 60400 recipes\n  - Flattened 60500 recipes\n  - Flattened 60600 recipes\n  - Flattened 60700 recipes\n  - Flattened 60800 recipes\n  - Flattened 60900 recipes\n  - Flattened 61000 recipes\n  - Flattened 61100 recipes\n  - Flattened 61200 recipes\n  - Flattened 61300 recipes\n  - Flattened 61400 recipes\n  - Flattened 61500 recipes\n  - Flattened 61600 recipes\n  - Flattened 61700 recipes\n  - Flattened 61800 recipes\n  - Flattened 61900 recipes\n  - Flattened 62000 recipes\n  - Flattened 62100 recipes\n  - Flattened 62200 recipes\n  - Flattened 62300 recipes\n  - Flattened 62400 recipes\n  - Flattened 62500 recipes\n  - Flattened 62600 recipes\n  - Flattened 62700 recipes\n  - Flattened 62800 recipes\n  - Flattened 62900 recipes\n  - Flattened 63000 recipes\n  - Flattened 63100 recipes\n  - Flattened 63200 recipes\n  - Flattened 63300 recipes\n  - Flattened 63400 recipes\n  - Flattened 63500 recipes\n  - Flattened 63600 recipes\n  - Flattened 63700 recipes\n  - Flattened 63800 recipes\n  - Flattened 63900 recipes\n  - Flattened 64000 recipes\n  - Flattened 64100 recipes\n  - Flattened 64200 recipes\n  - Flattened 64300 recipes\n  - Flattened 64400 recipes\n  - Flattened 64500 recipes\n  - Flattened 64600 recipes\n  - Flattened 64700 recipes\n  - Flattened 64800 recipes\n  - Flattened 64900 recipes\n  - Flattened 65000 recipes\n  - Flattened 65100 recipes\n  - Flattened 65200 recipes\n  - Flattened 65300 recipes\n  - Flattened 65400 recipes\n  - Flattened 65500 recipes\n  - Flattened 65600 recipes\n  - Flattened 65700 recipes\n  - Flattened 65800 recipes\n  - Flattened 65900 recipes\n  - Flattened 66000 recipes\n  - Flattened 66100 recipes\n  - Flattened 66200 recipes\n  - Flattened 66300 recipes\n  - Flattened 66400 recipes\n  - Flattened 66500 recipes\n  - Flattened 66600 recipes\n  - Flattened 66700 recipes\n  - Flattened 66800 recipes\n  - Flattened 66900 recipes\n  - Flattened 67000 recipes\n  - Flattened 67100 recipes\n  - Flattened 67200 recipes\n  - Flattened 67300 recipes\n  - Flattened 67400 recipes\n  - Flattened 67500 recipes\n  - Flattened 67600 recipes\n  - Flattened 67700 recipes\n  - Flattened 67800 recipes\n  - Flattened 67900 recipes\n  - Flattened 68000 recipes\n  - Flattened 68100 recipes\n  - Flattened 68200 recipes\n  - Flattened 68300 recipes\n  - Flattened 68400 recipes\n  - Flattened 68500 recipes\n  - Flattened 68600 recipes\n  - Flattened 68700 recipes\n  - Flattened 68800 recipes\n  - Flattened 68900 recipes\n  - Flattened 69000 recipes\n  - Flattened 69100 recipes\n  - Flattened 69200 recipes\n  - Flattened 69300 recipes\n  - Flattened 69400 recipes\n  - Flattened 69500 recipes\n  - Flattened 69600 recipes\n  - Flattened 69700 recipes\n  - Flattened 69800 recipes\n  - Flattened 69900 recipes\n  - Flattened 70000 recipes\n  - Flattened 70100 recipes\n  - Flattened 70200 recipes\n  - Flattened 70300 recipes\n  - Flattened 70400 recipes\n  - Flattened 70500 recipes\n  - Flattened 70600 recipes\n  - Flattened 70700 recipes\n  - Flattened 70800 recipes\n  - Flattened 70900 recipes\n  - Flattened 71000 recipes\n  - Flattened 71100 recipes\n  - Flattened 71200 recipes\n  - Flattened 71300 recipes\n  - Flattened 71400 recipes\n  - Flattened 71500 recipes\n  - Flattened 71600 recipes\n  - Flattened 71700 recipes\n  - Flattened 71800 recipes\n  - Flattened 71900 recipes\n  - Flattened 72000 recipes\n  - Flattened 72100 recipes\n  - Flattened 72200 recipes\n  - Flattened 72300 recipes\n  - Flattened 72400 recipes\n  - Flattened 72500 recipes\n  - Flattened 72600 recipes\n  - Flattened 72700 recipes\n  - Flattened 72800 recipes\n  - Flattened 72900 recipes\n  - Flattened 73000 recipes\n  - Flattened 73100 recipes\n  - Flattened 73200 recipes\n  - Flattened 73300 recipes\n  - Flattened 73400 recipes\n  - Flattened 73500 recipes\n  - Flattened 73600 recipes\n  - Flattened 73700 recipes\n  - Flattened 73800 recipes\n  - Flattened 73900 recipes\n  - Flattened 74000 recipes\n  - Flattened 74100 recipes\n  - Flattened 74200 recipes\n  - Flattened 74300 recipes\n  - Flattened 74400 recipes\n  - Flattened 74500 recipes\n  - Flattened 74600 recipes\n  - Flattened 74700 recipes\n  - Flattened 74800 recipes\n  - Flattened 74900 recipes\n  - Flattened 75000 recipes\n  - Flattened 75100 recipes\n  - Flattened 75200 recipes\n  - Flattened 75300 recipes\n  - Flattened 75400 recipes\n  - Flattened 75500 recipes\n  - Flattened 75600 recipes\n  - Flattened 75700 recipes\n  - Flattened 75800 recipes\n  - Flattened 75900 recipes\n  - Flattened 76000 recipes\n  - Flattened 76100 recipes\n  - Flattened 76200 recipes\n  - Flattened 76300 recipes\n  - Flattened 76400 recipes\n  - Flattened 76500 recipes\n  - Flattened 76600 recipes\n  - Flattened 76700 recipes\n  - Flattened 76800 recipes\n  - Flattened 76900 recipes\n  - Flattened 77000 recipes\n  - Flattened 77100 recipes\n  - Flattened 77200 recipes\n  - Flattened 77300 recipes\n  - Flattened 77400 recipes\n  - Flattened 77500 recipes\n  - Flattened 77600 recipes\n  - Flattened 77700 recipes\n  - Flattened 77800 recipes\n  - Flattened 77900 recipes\n  - Flattened 78000 recipes\n  - Flattened 78100 recipes\n  - Flattened 78200 recipes\n  - Flattened 78300 recipes\n  - Flattened 78400 recipes\n  - Flattened 78500 recipes\n  - Flattened 78600 recipes\n  - Flattened 78700 recipes\n  - Flattened 78800 recipes\n  - Flattened 78900 recipes\n  - Flattened 79000 recipes\n  - Flattened 79100 recipes\n  - Flattened 79200 recipes\n  - Flattened 79300 recipes\n  - Flattened 79400 recipes\n  - Flattened 79500 recipes\n  - Flattened 79600 recipes\n  - Flattened 79700 recipes\n  - Flattened 79800 recipes\n  - Flattened 79900 recipes\n  - Flattened 80000 recipes\n  - Flattened 80100 recipes\n  - Flattened 80200 recipes\n  - Flattened 80300 recipes\n  - Flattened 80400 recipes\n  - Flattened 80500 recipes\nFlattening complete. Converted 80600 nested lists to flat token lists.\n\nFinal result: 80600 ingredient token lists, 80600 step token lists\nExample ingredients tokens (first recipe): ['w', 'in', 't', 'er', ' s', 'q', 'u', 'a', 's', 'h']...\nExample steps tokens (first recipe): ['m', 'a', 'k', 'e ', 'a ', 'ch', 'o', 'i', 'c', ' ']...\n\nTransform complete!\n","output_type":"stream"}],"execution_count":38},{"id":"8f6e2f3b-f3a9-40ae-b6f5-53a53a596e6c","cell_type":"code","source":"data = np.load(\"/kaggle/working/bpe_80_tokenized_data.npz\", allow_pickle=True)\ningredients, steps= data['ingredients'], data['steps']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:19:14.385410Z","iopub.execute_input":"2025-05-09T14:19:14.385693Z","iopub.status.idle":"2025-05-09T14:19:15.953042Z","shell.execute_reply.started":"2025-05-09T14:19:14.385671Z","shell.execute_reply":"2025-05-09T14:19:15.952322Z"}},"outputs":[],"execution_count":39},{"id":"da8ea003-2336-407f-b27d-e46fef159268","cell_type":"code","source":"data = np.load(\"/kaggle/working/bpe_tokenized_data.npz\", allow_pickle=True)\ningredients, steps= data['ingredients'], data['steps']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:36:55.792523Z","iopub.execute_input":"2025-05-09T14:36:55.792800Z","iopub.status.idle":"2025-05-09T14:36:58.786457Z","shell.execute_reply.started":"2025-05-09T14:36:55.792781Z","shell.execute_reply":"2025-05-09T14:36:58.785653Z"}},"outputs":[],"execution_count":43},{"id":"80d5adb2-0442-456d-85fd-577b4d5e0777","cell_type":"code","source":"model = OptimizedBatchFeedForwardNN()\nmodel.fit(ingredients, steps, \n          embedding_dim=256, hidden_dim=128, \n          context=5, epochs=5, dropout=0.2, \n          batch_size=2048, learning_rate=1e-1,\n)\nmodel.save(\"/kaggle/working/bpe_64_optimized_ffnn_256_embedding_128_hidden_5_context_1e-1_lr.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:39:52.658957Z","iopub.execute_input":"2025-05-09T14:39:52.659259Z","iopub.status.idle":"2025-05-09T14:41:39.307208Z","shell.execute_reply.started":"2025-05-09T14:39:52.659238Z","shell.execute_reply":"2025-05-09T14:41:39.306581Z"}},"outputs":[{"name":"stdout","text":"Preparing unique context-grams for training...\nBuilding vocabulary...\nFound 816158 unique context-grams\nBuilding sample weights using context gram...\nVocab size: 64\nUnique training samples: 1898344\nData preparation took 43.17 seconds\nSample 0: [63 26 17  9  1] - 16\nSample 1: [26 17  9  1 16] - 5\nSample 2: [26 17  9  1 16] - 3\nSample 3: [26 17  9  1 16] - 2\nSample 4: [26 17  9  1 16] - 18\nSample 5: [26 17  9  1 16] - 0\nSample 6: [26 17  9  1 16] - 12\nSample 7: [26 17  9  1 16] - 19\nSample 8: [26 17  9  1 16] - 1\nSample 9: [26 17  9  1 16] - 11\nBuilding model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_11\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │          \u001b[38;5;34m81,920\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_11 (\u001b[38;5;33mFlatten\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_22               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │           \u001b[38;5;34m5,120\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ hidden (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m163,968\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_23               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ output (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">81,920</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_22               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_23               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m259,776\u001b[0m (1014.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">259,776</span> (1014.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m256,960\u001b[0m (1003.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256,960</span> (1003.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,816\u001b[0m (11.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> (11.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\nEpoch 1/5\n\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 16ms/step - loss: 0.5329 - sparse_categorical_accuracy: 0.1172 - val_loss: 2.1940 - val_sparse_categorical_accuracy: 0.3395 - learning_rate: 0.1000\nEpoch 2/5\n\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - loss: 0.4637 - sparse_categorical_accuracy: 0.1369 - val_loss: 2.1077 - val_sparse_categorical_accuracy: 0.3507 - learning_rate: 0.1000\nEpoch 3/5\n\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 0.4598 - sparse_categorical_accuracy: 0.1380 - val_loss: 2.3128 - val_sparse_categorical_accuracy: 0.3412 - learning_rate: 0.1000\nEpoch 4/5\n\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - loss: 0.4511 - sparse_categorical_accuracy: 0.1410 - val_loss: 2.0384 - val_sparse_categorical_accuracy: 0.3535 - learning_rate: 0.1000\nEpoch 5/5\n\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - loss: 0.4480 - sparse_categorical_accuracy: 0.1421 - val_loss: 2.0797 - val_sparse_categorical_accuracy: 0.3610 - learning_rate: 0.1000\nTraining took 63.33 seconds\nTotal process took 106.61 seconds\n","output_type":"stream"}],"execution_count":45},{"id":"deead1ad-126f-4d9f-89da-807e05ed81e1","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"732c490c-a511-4cd2-b04b-918319a99968","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e9046e59-6a7f-41b4-8340-833317f6f859","cell_type":"markdown","source":"#### Evaluate","metadata":{}},{"id":"8d84979e-6732-4cfb-985c-302b7f2c1aa1","cell_type":"code","source":"pred_steps = model.predict(ingredients[1])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-09T10:43:42.008Z"}},"outputs":[],"execution_count":null}]}